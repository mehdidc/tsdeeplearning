{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from time import sleep\n",
    "from IPython import display\n",
    "from utils import plot_decision_boundary\n",
    "from sklearn.base import BaseEstimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Logistic regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider we have a binary classification problem. Say we have a set of training examples $(X_i, y_i), i=1..N$, where :\n",
    "\n",
    "- $X_i$ is a vector of features of size J\n",
    "- $y_i=0..1$ is the class of the ith example\n",
    "\n",
    "Let's load an example dataset from scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "data = load_digits(2)\n",
    "X, y = data[\"data\"], data[\"target\"]\n",
    "N, J = X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(\"number of examples(N) : {0}\".format(N))\n",
    "print(\"number of features(J): {0}\".format(J))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(y)\n",
    "print(\"number of ones : {0}\".format(y.sum()))\n",
    "print(\"number of zeros : {0}\".format((1 - y).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic regression is a linear model for classification.\n",
    "Say x is an example, that is, a vector of size J and we want to compute the prediction, then :\n",
    "\n",
    "$p(y=1|x) = \\sigma(\\sum_{j=1}^J x_j w_j + b)$\n",
    "\n",
    "where $ \\sigma(a) = \\frac{1}{1 + exp(-a)}$\n",
    "\n",
    "and:\n",
    "\n",
    "if $p(y=1|x) > 0.5$ we predict 1, else we predict 0\n",
    "\n",
    "The goal is learn  the values for $w$ and $b$ from data to minimize [cross entropy](https://en.wikipedia.org/wiki/Cross_entropy), a way to evaluate the difference between what we predict and the \"truth\", assuming y given x follows a bernoulli probability distribution.\n",
    "\n",
    "Let's try it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import cross_val_score, train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = LogisticRegression()\n",
    "# splitting train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "clf.fit(X_train, y_train)\n",
    "# accuracy on test set\n",
    "print((clf.predict(X_test)==y_test).mean())\n",
    "\n",
    "# outputting the parameters\n",
    "b = clf.intercept_[0]\n",
    "w = clf.coef_[0]\n",
    "\n",
    "print(\"b : {0}\".format(b))\n",
    "print(\"w : {0}\".format(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize what logistic regression does for a 2D problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=1000, centers=2, n_features=2,\n",
    "                  random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=y, marker='o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This problem is linearly separable so logistic regression works well, here is the decision boundary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(X, y)\n",
    "plot_decision_boundary(clf, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if the problem is not linearly separable ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_gaussian_quantiles\n",
    "X, y = make_gaussian_quantiles(n_samples=500, n_features=2, n_classes=2)\n",
    "plt.scatter(X[:, 0], X[:, 1], marker='o', c=y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(X, y)\n",
    "plot_decision_boundary(clf, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we solve that ? we can project our features in a feature space of higher dimension and then apply logistic regression on the new feature space.\n",
    "\n",
    "The model becomes:\n",
    "\n",
    "$p(y=1|x) = \\sigma(\\sum_{j=1}^J \\Phi_j(x) w_j + b)$\n",
    "\n",
    "where $\\Phi_j(x)$ is the j-th feature in the new feature space.\n",
    "\n",
    "how do we choose the features in the new space, we have a variety of possbilities.\n",
    "We can for instance use polynomial expansions where we fix the maximum degree to\n",
    "some number, P.\n",
    "\n",
    "Let's try $P=2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline = make_pipeline(\n",
    "    PolynomialFeatures(degree=2, interaction_only=False, include_bias=False),\n",
    "    LogisticRegression()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline.fit(X, y)\n",
    "plot_decision_boundary(pipeline, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem of this approach is that it is problem dependent, there is no reason why\n",
    "polynomial expansions would work for any problem. \n",
    "\n",
    "For each problem, we have to design a feature space which makes the problem\n",
    "linearly separable or at least easier, this part of the process is \n",
    "called **feature engineering** and it is a difficult problem.\n",
    "\n",
    "Is there a way to automatically learn that feature space from the data ?\n",
    "this is what deep learning is about, learning $\\Phi$ from data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedforward neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![nn](nn.png)\n",
    "\n",
    "**Reference**  http://www.rsipvision.com/exploring-deep-learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "With neural networks, units are organized in layers, you can think of each **layer** as a representation of the inputs, just like we did with polynomials, except that these representations are learned from data. \n",
    "\n",
    "In this view, the **input layer** and the **output layer** are no longer special layers, but also representations of the same data.\n",
    "\n",
    "- The **input layer** are the raw representation of the data\n",
    "- The **output layer** can also be seen as a representation of the same data, except that it is a very abstract representation (for instance, class labels)\n",
    "- **Hidden layers** are representations of intermediate levels of abstraction going from less abstract to more abstract as we go from the input layer to the output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In feedforward neural networks, each layer is a linear transformation of the previous layer for which we apply a non-linear function called the **activation function**. Or put it in another way, each unit of each layer is a linear combination of all the units of the previous layer for which we apply a non-linear function called the **activation function**.\n",
    "\n",
    "A feedforward neural network can thus be easily expressed using products of matrices, for instance let's say we have 2 hidden layers.\n",
    "\n",
    "let's say $x$ is an input vector:\n",
    "\n",
    "- $h_1 = g^{(1)}(W^{(1)}x + b^{(1)})$ is the first hidden layer\n",
    "- $h_2 = g^{(2)}(W^{(2)} h_1 + b^{(2)})$ is the second hidden layer\n",
    "- $y = g^{(3)}(W^{(3)}h_2 + b^{(3)})$ is the output layer\n",
    "\n",
    "where:\n",
    "\n",
    "- $g^{(l)}$ is the activation function of the layer l\n",
    "- $W^{(l)}$ are called the weights and they determine the linear transformation we use from one layer to another"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Let's build a simple example :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W1 = np.random.uniform(0, 1, size=(X.shape[1], 100)) # 100 units in the first layer\n",
    "W2 = np.random.uniform(0, 1, size=(100, 50)) # 50 units in the second layer\n",
    "W3 = np.random.uniform(0, 1, size=(50, 1)) # 1 unit in the output layer\n",
    "\n",
    "b1= np.random.uniform(0, 1, size=(100,))\n",
    "b2 = np.random.uniform(0, 1, size=(50))\n",
    "b3 = np.random.uniform(0, 1, size=(1,))\n",
    "\n",
    "g = np.tanh # the activation function is the same for all the layers\n",
    "\n",
    "h1 = g(np.dot(X, W1) + b1)\n",
    "h2 = g(np.dot(h1, W2) + b2)\n",
    "y = g(np.dot(h2, W3) + b3)\n",
    "\n",
    "print(X.shape, h1.shape, h2.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a **task** and a **dataset**, for instance class prediction of insects, how do we train a neural network ?\n",
    "\n",
    "First, we specify what we want from the model through the **loss function**, also known as **objective function**.\n",
    "Usually it describes the difference between what what we predict and the \"truth\" and we want thus to minimize the **loss function**.\n",
    "\n",
    "For class prediction, the loss function could be the number of misclassifications on the training data.\n",
    "The problem with this loss function is that it is not smooth, and neural networks need smooth loss functions\n",
    "to be trained. We rather employ surrogates of the misclassification loss function and monitor the\n",
    "real loss function we want to minimize in training/valid/test data.\n",
    "\n",
    "For binary classification, the simplest loss function we can use is the mean squared error :\n",
    "\n",
    "$$L(X, y) = \\frac{1}{2N}\\sum_{i=1}^N (f(X_i) - y_i)^2$$\n",
    "\n",
    "where f is a function that computes the output of the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ## Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having the dataset and having defined the loss function, how do we \n",
    "actually train the weights of the neural network ?\n",
    "\n",
    "The usual algorithm for training neural networks is gradient descent (and its variants), which requires\n",
    "us to compute the derivatives of the loss function with respect to the parameters of the neural network, which\n",
    "are the weights and the biases.\n",
    "\n",
    "**Backpropagation** is an algorithm for computing the partial derivatives  of the loss function with respect\n",
    "to the parameters of any network defined by a Directed Acyclic Graph (DAG), it is basically\n",
    "a recursive application of the chain rule starting from the outputs and going backwards through\n",
    "the edges of the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![bp](backprop.png)\n",
    "\n",
    "Let's take a simple example. the above graph is a computation graph, each node is a function\n",
    "of its predecessors, the direction is the way we procede to compute the outputs, so\n",
    "we start at **r** and **t**, compute **x** based on **r** and **t**, compute **y** based on **r** and **t** then finally\n",
    "compute **u** based on **x** and **y**.\n",
    "\n",
    "The chain rule for partial derivatives tells us that :\n",
    "\n",
    "$$\\frac{\\partial u}{\\partial r} = \\frac{\\partial u}{\\partial x}\\frac{\\partial x}{\\partial r} + \\frac{\\partial u}{\\partial y}\\frac{\\partial y}{\\partial r}$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\frac{\\partial u}{\\partial t} = \\frac{\\partial u}{\\partial x}\\frac{\\partial x}{\\partial t} + \\frac{\\partial u}{\\partial y}\\frac{\\partial y}{\\partial t}$$\n",
    "\n",
    "and\n",
    "\n",
    "we have the expressions of $$\\frac{\\partial u}{\\partial x}$$ and\n",
    "$$\\frac{\\partial u}{\\partial y}$$  because they are direct predecessors of **u**.\n",
    "\n",
    "\n",
    "Now suppose the node **u** is representing the loss function and the dependency between nodes is parametrized each one by \n",
    "a parameter:\n",
    "\n",
    "![bp](backprop2.png)\n",
    "\n",
    "\n",
    "then as seen above backprop gives us a way to compute the partial derivative of the loss function with respect to any node,\n",
    "for instance for **r** : $\\frac{\\partial u}{\\partial r}$, based on the partial derivatives of the loss function with respect to its successors,\n",
    "for **r** it is **x** and **y**, thus $\\frac{\\partial u}{\\partial x}$ and $\\frac{\\partial u}{\\partial y}$.\n",
    "\n",
    "So partial derivatives of the loss function **u** with respect to the nodes **x, y, r, t** are computed like the following, first we compute $\\frac{\\partial u}{\\partial x}$  and $\\frac{\\partial u}{\\partial y}$ directly because they are direct predecessors of **u**, then we go backward into the predecessors of **x** and **y** and compute $\\frac{\\partial u}{\\partial r}$ and $\\frac{\\partial t}{\\partial y}$ based on $\\frac{\\partial u}{\\partial x}$ and $\\frac{\\partial u}{\\partial y}$. This procedure (backpropagation) is generalizable to any Direct Acyclic Graph (DAG).\n",
    "\n",
    "Now, having computed the partial derivative of the loss function with respect to the all the nodes with backpropagation, the derivatives of the loss function with respect to the parameters is easily obtained :\n",
    "\n",
    "\n",
    "$$\\frac{\\partial u}{\\partial W_{rx}} =\\frac{\\partial u}{\\partial x}\\frac{\\partial x}{\\partial W_{rx}}$$\n",
    "$$\\frac{\\partial u}{\\partial W_{ry}} =\\frac{\\partial u}{\\partial y}\\frac{\\partial y}{\\partial W_{ry}}$$\n",
    "\n",
    "\n",
    "$$\\frac{\\partial u}{\\partial W_{tx}} =\\frac{\\partial u}{\\partial x}\\frac{\\partial x}{\\partial W_{tx}}$$\n",
    "$$\\frac{\\partial u}{\\partial W_{ty}} =\\frac{\\partial u}{\\partial y}\\frac{\\partial y}{\\partial W_{ty}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see what backpropagation gives for feedforward neural networks.\n",
    "\n",
    "We consider a neural network with **L** layers and we illustrate below two intermediate layers **l** and **l+1** and compute the partial derivatives of one layer given the partial derivatives of the next layer using backpropagation as given above.\n",
    "\n",
    "![backprop-nnet.png](backprop-nnet.png)\n",
    "\n",
    "the above computational graph translates into:\n",
    "\n",
    "$$E^{(l+1)}_j = \\sum_i H^{(l)}_i W^{(l)}_{i, j}$$\n",
    "\n",
    "and\n",
    "\n",
    "$$H^{(l+1)}_j = g^{(l+1)}(E^{(l+1)}_j)$$\n",
    "\n",
    "where $g^{(l)}$ is the activation function of the layer **l + 1**.\n",
    "\n",
    "The vectorized version is:\n",
    "\n",
    "$$E^{(l+1)} = H^{(l)}W^{(l)}$$\n",
    "\n",
    "and\n",
    "\n",
    "$$H^{(l+1)} = g^{(l+1)}(E^{(l+1)})$$\n",
    "\n",
    "Now, if **L** is the loss function node, then:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial H^{(l)}_i} = \\sum_j \\frac{\\partial L}{\\partial E^{(l+1)}_j} W^{(l)}_{i,j}$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial E^{(l+1)}_j} = \\frac{\\partial L}{H^{(l+1)}_j}g'( E^{(l+1)}_j)$$\n",
    "\n",
    "and the vectorized version is:\n",
    "\n",
    "$\\frac{\\partial L}{\\partial H^{(l)}} = \\frac{\\partial L}{\\partial E^{(l+1)}}(W^{(l)})^T$ **(1)**\n",
    "\n",
    "$\\frac{\\partial L}{\\partial E^{(l+1)}} = \\frac{\\partial L}{\\partial H^{(l+1)}} * g'( E^{(l+1)})$ **(2)**\n",
    "\n",
    "the two above equations can be merged :\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial H^{(l)}} = \\frac{\\partial L}{\\partial H^{(l+1)}}g'( E^{(l+1)})(W^{(l)})^T$$\n",
    "\n",
    "now to find the derivatives of the loss with respect to $W^{(l)}$, it is simple:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial W^{(l)}_{i, j}} = \\frac{\\partial L}{\\partial E^{(l + 1)}_j}\\frac{\\partial E^{(l + 1)}_j}{\\partial W_{i, j}} = \\frac{\\partial L}{\\partial E^{(l + 1)}_j}H^{(l)}_i$$\n",
    "\n",
    "and the vectorized form is:\n",
    "\n",
    "$\\frac{\\partial L}{\\partial W^{(l)}} = (H^{(l)})^T\\frac{\\partial L}{\\partial E^{(l + 1)}}  $ **(3)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use directly equations **(1)**, **(2)** and **(3)** to train a neural network, having defined the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use equations **(1)**, **(2)** and **(3)** to train a basic neural network with 2 layers.\n",
    "\n",
    "We will solve the same problem than before, so it is a classification problem and we will use the squared error as a loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_gaussian_quantiles\n",
    "X, y = make_gaussian_quantiles(n_samples=500, n_features=2, n_classes=2)\n",
    "plt.scatter(X[:, 0], X[:, 1], marker='o', c=y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see that y is a binary variable, one way to predict values between 0 and 1 is to use the sigmoid  function $\\sigma$ as an activation function on the output layer, like with logisitic regression.\n",
    "\n",
    "Recall that : where $ \\sigma(a) = \\frac{1}{1 + exp(-a)}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = np.arange(-5, 5, 0.1)\n",
    "plt.plot(a, 1./(1 + np.exp(-a)))\n",
    "plt.title(\"$\\sigma$ (sigmoid) activation function\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the squared error as a loss function, defined by :\n",
    "    \n",
    "$$L(X, y) = \\frac{1}{2}\\sum_{i=1}^N (f(X_i) - y_i)^2$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The direct predecessors of the loss function node are the values of the output layer, so we first compute the derivatives of the loss function with respect to the output layer.\n",
    "Assuming $O$ is the output layer,\n",
    "\n",
    "$$\\frac{\\partial{L}}{\\partial O} = (O - Y)$$\n",
    "\n",
    "where $Y$ is the dataset \"true\" classes and $O$ the predicted ones, both are vectors\n",
    "of size $N$, each element representing one example.\n",
    "\n",
    "Then the derivation for the other layers is obtained directly using equations **(1)**, **(2)** and the derivation for the parameters from equation **(3)**. In equation **(3)** we did not include the derivatives for of the loss function with respect to the biases $b^{(l)}$, but it is straightforward and can be implied from equation **(3)**. You can think of the biases as part of the weights but multiplying inputs with the value of **1**, thus for biases,\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial b_j^{(l)}} = \\frac{\\partial L}{\\partial E^{(l + 1)}_j}\\frac{\\partial E^{(l + 1)}_j}{\\partial b_j^{(l)}} = \\frac{\\partial L}{\\partial E^{(l + 1)}_j}$$\n",
    "\n",
    "and the vectorized form :\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial b^{(l)}} = \\frac{\\partial L}{\\partial E^{(l + 1)}}  $$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our neural network is defined like this again:\n",
    "\n",
    "- $h_1 = g^{(1)}(W^{(1)}x + b^{(1)})$ is the first hidden layer\n",
    "- $h_2 = g^{(2)}(W^{(2)} h_1 + b^{(2)})$ is the second hidden layer\n",
    "- $y = g^{(3)}(W^{(3)}h_2 + b^{(3)})$ is the output layer\n",
    "\n",
    "where\n",
    "\n",
    "$$g^{(1)}(a) = g^{(2)}(a) = g^{(3)}(a) = \\sigma(a) =  \\frac{1}{1 + exp(-a)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now we initialize the parameters randomly, we will talk more about initialization and its importance later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W1 = np.random.uniform(-0.001, 0.001, size=(X.shape[1], 100)) # 100 units in the first layer\n",
    "W2 = np.random.uniform(-0.01, 0.01, size=(100, 50)) # 50 units in the second layer\n",
    "W3 = np.random.uniform(-0.1, 0.1, size=(50, 1)) # 1 unit in the output layer\n",
    "\n",
    "b1= np.zeros(100,)\n",
    "b2 = np.zeros((50))\n",
    "b3 = np.zeros((1,))\n",
    "\n",
    "g = lambda x:1./(1 + np.exp(-x)) # the activation function is the same for all the layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let's try a forward pass and check the initial value of the loss function\n",
    "N = y.shape[0]\n",
    "e1 = np.dot(X, W1) + b1\n",
    "h1 = g(e1)\n",
    "e2 = np.dot(h1, W2) + b2\n",
    "h2 = g(e2)\n",
    "e3 = np.dot(h2, W3) + b3\n",
    "o = g(e3)\n",
    "\n",
    "print(\"X:{0}, h1:{1}, h2:{2}, y:{3}\".format(X.shape, h1.shape, h2.shape, y.shape))\n",
    "\n",
    "# initial accuracy\n",
    "acc =  ((o>0.5)==y).mean()\n",
    "print(\"accuracy : {0}\".format(acc))\n",
    "# initial loss function value\n",
    "L = (0.5 * ((y[:, np.newaxis] - o)**2)).sum()\n",
    "print(\"loss : {0}\".format(L))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply backpropagation to get the derivatives of the loss with respect to the nodes\n",
    "\n",
    "notice that for sigmoid, $$g'(a) = g(a) * (1 - g(a))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d_o = (o - y[:, np.newaxis] )\n",
    "d_e3 = d_o * o * (1 - o) # equation 2 + using the property of the sigmoid described just above\n",
    "d_h2 = np.dot(d_e3, W3.T) # equation 1\n",
    "d_e2 = d_h2 * h2 * (1 - h2) # equation 2 + + using the property of the sigmoid described just above\n",
    "d_h1 = np.dot(d_e2, W2.T) # equation 1\n",
    "d_e1 = d_h1 * h1 * (1 - h1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute the derivatives with respect to the parameters using equation **3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d_W3 = np.dot(h2.T, d_e3)\n",
    "d_W2 = np.dot(h1.T, d_e2)\n",
    "d_W1 = np.dot(X.T, d_e1)\n",
    "\n",
    "d_b3 = d_e3.sum(axis=0)\n",
    "d_b2 = d_e2.sum(axis=0)\n",
    "d_b1 = d_e1.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# gradient check\n",
    "\n",
    "# Make sure the gradients are correct, use finite differences to compute\n",
    "# gradients numeriacally and compare it to the gradients obtained by\n",
    "# backpropgation\n",
    "\n",
    "def get_loss(X):\n",
    "    e1 = np.dot(X, W1) + b1\n",
    "    h1 = g(e1)\n",
    "    e2 = np.dot(h1, W2) + b2\n",
    "    h2 = g(e2)\n",
    "    e3 = np.dot(h2, W3) + b3\n",
    "    o = g(e3)\n",
    "    L = 0.5 * ((y[:, np.newaxis] - o)**2)\n",
    "    return L.sum()\n",
    "\n",
    "params = [W1, W2, W3, b1, b2, b3]\n",
    "d_params = [d_W1, d_W2, d_W3, d_b1, d_b2, d_b3]\n",
    "epsilon = 1e-6\n",
    "for p, d_p in zip(params, d_params):\n",
    "    p_flat = p.reshape((-1,))\n",
    "    d_p_flat = d_p.reshape((-1,))\n",
    "    for i in range(p_flat.shape[0]):\n",
    "        # use finite difference to compute gradients numerically\n",
    "        # and compare it with gradients computed from backprop\n",
    "        val = p_flat[i]\n",
    "        p_flat[i] = val + epsilon\n",
    "        La = get_loss(X)\n",
    "        p_flat[i] = val -  epsilon\n",
    "        Lb = get_loss(X)\n",
    "        p_flat[i] = val\n",
    "        grad = (La - Lb) / (2 * epsilon)\n",
    "        assert np.abs(grad - d_p_flat[i]) <= 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent tells us that to minimize the loss function **L** with respect to some parameters $\\Theta$ , we have to follow in each iteration the direction  defined by the **gradients** : -$\\frac{\\partial{L}}{\\partial{\\Theta}}$ scaled by a hyper-parameter $\\alpha$ which defines the step size at each iteration and is called the **learning rate**.\n",
    "\n",
    "\n",
    "Gradient descent can only converge to a local minimum. If the **learning rate** is too small, convergence towards the minimum is slow, but if the **learning rate** is too big, there is a risk that gradient descent diverges, so one seeks for the biggest value of the **learning rate** that do not diverge. Neural networks are very sensitive to the value **learning rate**, so it is important to tune it well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do one step of gradient descent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alpha = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W3 -= alpha * d_W3\n",
    "W2 -= alpha * d_W2\n",
    "W1 -= alpha * d_W1\n",
    "b3 -= alpha * d_b3\n",
    "b2 -= alpha * d_b2\n",
    "b1 -= alpha * d_b1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and recompute the loss function value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = y.shape[0]\n",
    "e1 = np.dot(X, W1) + b1\n",
    "h1 = g(e1)\n",
    "e2 = np.dot(h1, W2) + b2\n",
    "h2 = g(e2)\n",
    "e3 = np.dot(h2, W3) + b3\n",
    "o = g(e3)\n",
    "\n",
    "L = 0.5 * ((y[:, np.newaxis] - o)**2)\n",
    "print(\"loss : \", L.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now put everything together and apply gradient descent for several iterations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alpha = 0.03\n",
    "nb_epochs = 2000\n",
    "\n",
    "# Initialization\n",
    "nb_hidden1 = 10\n",
    "nb_hidden2 = 10\n",
    "\n",
    "W1 = np.random.uniform(-0.0001, 0.0001, size=(X.shape[1], nb_hidden1))\n",
    "W2 = np.random.uniform(-0.001, 0.001, size=(nb_hidden1, nb_hidden2)) # 50 units in the second layer\n",
    "W3 = np.random.uniform(-0.01, 0.01, size=(nb_hidden2, 1)) # 1 unit in the output layer\n",
    "\n",
    "b1= np.zeros(nb_hidden1,)\n",
    "b2 = np.zeros((nb_hidden2))\n",
    "b3 = np.zeros((1,))\n",
    "\n",
    "g = lambda x:1./(1 + np.exp(-x)) # the activation function is the same for all the layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "L_monitor = []\n",
    "acc_monitor = []\n",
    "for epoch in range(nb_epochs):\n",
    "    N = X_.shape[0]\n",
    "    # forward pass\n",
    "    e1 = np.dot(X_, W1) + b1\n",
    "    h1 = g(e1)\n",
    "    e2 = np.dot(h1, W2) + b2\n",
    "    h2 = g(e2)\n",
    "    e3 = np.dot(h2, W3) + b3\n",
    "    o = g(e3)\n",
    "    acc = ((o[:, 0]>0.5)==y).mean()\n",
    "    acc_monitor.append(acc)\n",
    "    print(\"accuracy : \", acc)\n",
    "    L = 0.5 * ((y_[:, np.newaxis] - o)**2)\n",
    "    print(\"loss : \", L.sum())\n",
    "    L_monitor.append(L.sum())\n",
    "    # backward pass\n",
    "    d_o = (o - y[:, np.newaxis])\n",
    "    d_e3 = d_o * o * (1 - o) # equation 2 + using the property of the sigmoid described just above\n",
    "    d_h2 = np.dot(d_e3, W3.T) # equation 1\n",
    "    d_e2 = d_h2 * h2 * (1 - h2) # equation 2 + + using the property of the sigmoid described just above\n",
    "    d_h1 = np.dot(d_e2, W2.T) # equation 1\n",
    "    d_e1 = d_h1 * h1 * (1 - h1)\n",
    "    # gradients\n",
    "    d_W3 = np.dot(h2.T, d_e3)\n",
    "    d_W2 = np.dot(h1.T, d_e2)\n",
    "    d_W1 = np.dot(X_.T, d_e1)\n",
    "    \n",
    "    d_b3 = d_e3.sum(axis=0)\n",
    "    d_b2 = d_e2.sum(axis=0)\n",
    "    d_b1 = d_e1.sum(axis=0)\n",
    "    # update\n",
    "\n",
    "    W3 -= alpha * d_W3\n",
    "    W2 -= alpha * d_W2\n",
    "    W1 -= alpha * d_W1\n",
    "    b3 -= alpha * d_b3\n",
    "    b2 -= alpha * d_b2\n",
    "    b1 -= alpha * d_b1\n",
    "\n",
    "plt.plot(L_monitor)\n",
    "plt.title(\"loss\")\n",
    "plt.show()\n",
    "plt.plot(acc_monitor)\n",
    "plt.title(\"acc\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NeuralNet(BaseEstimator):\n",
    "    \n",
    "    def predict(self, X):\n",
    "        e1 = np.dot(X, W1) + b1\n",
    "        h1 = g(e1)\n",
    "        e2 = np.dot(h1, W2) + b2\n",
    "        h2 = g(e2)\n",
    "        e3 = np.dot(h2, W3) + b3\n",
    "        o = g(e3)\n",
    "        y = 1 * (o > 0.5)\n",
    "        return y[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = NeuralNet()\n",
    "plot_decision_boundary(model, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, here we used **squared error** as a loss function for its simplicity but we note that for classification it is rather the [**cross entropy**](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html) loss function which is used in general along with a [**softmax**](https://en.wikipedia.org/wiki/Softmax_function) activation function in the output layer rather than **sigmoid**. \n",
    "\n",
    "The [**softmax**](https://en.wikipedia.org/wiki/Softmax_function) activation function in the output layer converts a set of predictions (one prediction per class) to probabilties, corresponding to a **multinomial distribution**.  \n",
    "\n",
    "**Cross entropy** penalizes bad predictions more heavily than **squared error**, say for an example the true class is the j-th one and $p_j$ is the predicted probability of the class j (obtained from softmax), then **cross entropy** is for this case : $-log p_j$ which goes to infinity as $p_j$ goes to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural  networks libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nolearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will get introduced to one of the neural network libraries and introduce at the same time new concepts.\n",
    "\n",
    "[**Nolearn**](https://github.com/dnouri/nolearn/) is a high level neural network library, based on [**Lasagne**](https://github.com/Lasagne/Lasagne), which provides a [scikit-learn](scikit-learn.org) like **NeuralNet** class with fit, predict etc. [**Lasagne**](https://github.com/Lasagne/Lasagne) itself is based on [**theano**](https://github.com/Theano/).\n",
    "\n",
    "[**theano**](https://github.com/Theano/) is not  a library for neural networks only, it is a library which allow you\n",
    "to manipulate a symbolic computational graph. \n",
    "\n",
    "For instance it allows you to write\n",
    "the loss function as a symbolic variable and then get the gradients with respect to the parameters automatically, after that you can define functions which do some \n",
    "computations based on these symbolic variables, like for instance updating the parameters of your model based on the gradients of the loss function with respect to the parameters. These functions are compiled to a fast C code or a CUDA code if you have an NVIDIA GPU card. \n",
    "\n",
    "There are a lot of high level libraries for neural networks which uses theano and [**Lasagne**](https://github.com/Lasagne/Lasagne) is one of them. [**Nolearn**](https://github.com/dnouri/nolearn/) provides us a class with a scikit-learn interface based on [**Lasagne**](https://github.com/Lasagne/Lasagne)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train  a **feedforward neural network** with **nolearn** for classifying insects !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by loading the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = np.load(\"train_64x64_subsampled.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, y = data[\"X\"], data[\"y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 64, 64, 3)\n",
      "(500,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X are the examples and y the labels.\n",
    "We see that X is a 4D tensor where :\n",
    "\n",
    "- the first dimension is the examples\n",
    "- the second is the height\n",
    "- the third is the width \n",
    "- the fourth (and last) are color channels (RGB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 12288)\n"
     ]
    }
   ],
   "source": [
    "X = X.astype(np.float32) # For GPUs, theano excpects float32 inputs\n",
    "X_vectorized = X.reshape((X.shape[0], -1))\n",
    "print(X_vectorized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nolearn.lasagne import NeuralNet, BatchIterator\n",
    "from lasagne import layers, nonlinearities, updates, init, objectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net = NeuralNet(\n",
    "    # Define the architecture here\n",
    "    layers=[\n",
    "            ('input', layers.InputLayer), \n",
    "            ('hidden1', layers.DenseLayer),\n",
    "            ('output', layers.DenseLayer),\n",
    "    ],\n",
    "    # Layers parameters:\n",
    "    input_shape=(None, X_vectorized.shape[1]), # Number of input features\n",
    "    \n",
    "    hidden1_num_units=500,  # number of units in 1st hidden layer\n",
    "    hidden1_W=init.Uniform(-0.001, 0.001), # Initialization of weights of 1st hidden layer\n",
    "    hidden1_nonlinearity=nonlinearities.rectify, # Activation function of 1st hidden layer\n",
    "\n",
    "    output_num_units=18,  # 18 classes\n",
    "    output_W=init.Uniform(-0.001, 0.001), #Initialization of weights of output layer\n",
    "    output_nonlinearity=nonlinearities.softmax, # Softmax as activation function for output layer\n",
    "    \n",
    "    # Loss function\n",
    "    objective_loss_function=objectives.categorical_crossentropy,\n",
    "    eval_size=0.2, # Validation set size to monitor and apply early stopping\n",
    "    \n",
    "    # Optimization method:\n",
    "    update=updates.sgd, # The optimization algorithm is stochastic gradient descent (SGD)\n",
    "    update_learning_rate=0.01, # The global learning rate of all the parameters for SGD,\n",
    "    batch_iterator_train=BatchIterator(batch_size=128), # mini-batch size\n",
    "    \n",
    "    use_label_encoder=True, # Converts labels of any kind to integers\n",
    "    max_epochs=15,  # we want to train this many epochs\n",
    "    verbose=1, # To monitor training at each epoch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = NeuralNet(\n",
    "    # Define the architecture here\n",
    "    layers=[\n",
    "            ('input', layers.InputLayer), \n",
    "            ('hidden1', layers.DenseLayer),\n",
    "            ('output', layers.DenseLayer),\n",
    "    ],\n",
    "    # Layers parameters:\n",
    "    input_shape=(None, X_vectorized.shape[1]), # Number of input features\n",
    "    \n",
    "    hidden1_num_units=100,  # number of units in 1st hidden layer\n",
    "    hidden1_nonlinearity=nonlinearities.sigmoid,\n",
    "    hidden1_W=init.Uniform((-0.01, 0.01)),\n",
    "    \n",
    "    output_num_units=18,  # 18 classes    \n",
    "    output_W=init.Uniform((-0.01, 0.01)),\n",
    "    output_nonlinearity=nonlinearities.softmax,\n",
    "\n",
    "    # Optimization method:\n",
    "    update=updates.sgd, # The optimization algorithm is stochastic gradient descent (SGD)\n",
    "    update_learning_rate=0.1, # The global learning rate of all the parameters for SGD,\n",
    "    batch_iterator_train=BatchIterator(batch_size=100), # mini-batch size\n",
    "    \n",
    "    use_label_encoder=True, # Converts labels of any kind to integers\n",
    "    max_epochs=300,  # we want to train this many epochs\n",
    "    verbose=1, # To monitor training at each epoch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "net.fit(X_vectorized, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training seems slow, we are even not capable of fitting training data. What is the reason behind this slowness ?\n",
    "First, let's check scaling of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(X_vectorized.min(), X_vectorized.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because input have large values the values of the hidden layer before applying sigmoid have a big magnitude, so the values after applying sigmoid are mostly either close to 0 or close to 1, so they are **saturated**. One way to prevent this is to rescale the inputs to a range which is **compatible** with the range of the activation function, here the sigmoid. \n",
    "\n",
    "One way to do that is to force the features to be between the min and the max, for this specific case we can just divide by 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = NeuralNet(\n",
    "    # Define the architecture here\n",
    "    layers=[\n",
    "            ('input', layers.InputLayer), \n",
    "            ('hidden1', layers.DenseLayer),\n",
    "            ('output', layers.DenseLayer),\n",
    "    ],\n",
    "    # Layers parameters:\n",
    "    input_shape=(None, X_vectorized.shape[1]), # Number of input features\n",
    "    \n",
    "    hidden1_num_units=100,  # number of units in 1st hidden layer\n",
    "    hidden1_nonlinearity=nonlinearities.sigmoid,\n",
    "    hidden1_W=init.Uniform((-0.01, 0.01)),\n",
    "    \n",
    "    output_num_units=18,  # 18 classes    \n",
    "    output_W=init.Uniform((-0.01, 0.01)),\n",
    "    output_nonlinearity=nonlinearities.softmax,\n",
    "\n",
    "    # Optimization method:\n",
    "    update=updates.sgd, # The optimization algorithm is stochastic gradient descent (SGD)\n",
    "    update_learning_rate=0.1, # The global learning rate of all the parameters for SGD,\n",
    "    batch_iterator_train=BatchIterator(batch_size=100), # mini-batch size\n",
    "    \n",
    "    use_label_encoder=True, # Converts labels of any kind to integers\n",
    "    max_epochs=300,  # we want to train this many epochs\n",
    "    verbose=1, # To monitor training at each epoch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net.fit(X_vectorized/255., y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the difference between the two. Notice also that in the first case we are **underfitting** and in the second case we are **overfitting**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are other ways of preprocessing the data, in (Efficient Backprop, Yann Lecun, 1998), they advice to make the average of each input feature close to zero, to rescale input features to the same scale and make the input features uncorrelated. Making average of each input close to zero and rescaling them to the same scale can be implemetend using [**StandardScaler()**](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) , linear decorrelation can be implemented by a [**PCA**](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More than that, in (Efficient Backprop, Yann Lecun, 1998), they advice to make the average of each layer to be close to zero, and thus this make classical **sigmoid** not **recommanded** because it is not centered around zero, **tanh** which is a rescaled version of sigmoid, is preferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = np.arange(-10, 10, 0.1)\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(a, np.tanh(a))\n",
    "plt.title(\"tanh activation function\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(a, 1./(1. + np.exp(-a)))\n",
    "plt.title(\"sigmoid activation function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net = NeuralNet(\n",
    "    # Define the architecture here\n",
    "    layers=[\n",
    "            ('input', layers.InputLayer), \n",
    "            ('hidden1', layers.DenseLayer),\n",
    "            ('output', layers.DenseLayer),\n",
    "    ],\n",
    "    # Layers parameters:\n",
    "    input_shape=(None, X_vectorized.shape[1]), # Number of input features\n",
    "    \n",
    "    hidden1_num_units=100,  # number of units in 1st hidden layer\n",
    "    hidden1_nonlinearity=nonlinearities.tanh,\n",
    "    hidden1_W=init.Uniform((-0.01, 0.01)),\n",
    "    \n",
    "    output_num_units=18,  # 18 classes    \n",
    "    output_W=init.Uniform((-0.01, 0.01)),\n",
    "    output_nonlinearity=nonlinearities.softmax,\n",
    "\n",
    "    # Optimization method:\n",
    "    update=updates.sgd, # The optimization algorithm is stochastic gradient descent (SGD)\n",
    "    update_learning_rate=0.1, # The global learning rate of all the parameters for SGD,\n",
    "    batch_iterator_train=BatchIterator(batch_size=100), # mini-batch size\n",
    "    \n",
    "    use_label_encoder=True, # Converts labels of any kind to integers\n",
    "    max_epochs=30,  # we want to train this many epochs\n",
    "    verbose=1, # To monitor training at each epoch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mehdi/work/code/lasagne/lasagne/init.py:91: UserWarning: The uniform initializer no longer uses Glorot et al.'s approach to determine the bounds, but defaults to the range (-0.01, 0.01) instead. Please use the new GlorotUniform initializer to get the old behavior. GlorotUniform is now the default for all layers.\n",
      "  warnings.warn(\"The uniform initializer no longer uses Glorot et al.'s \"\n",
      "/home/mehdi/work/code/lasagne/lasagne/layers/helper.py:69: UserWarning: get_all_layers() has been changed to return layers in topological order. The former implementation is still available as get_all_layers_old(), but will be removed before the first release of Lasagne. To ignore this warning, use `warnings.filterwarnings('ignore', '.*topo.*')`.\n",
      "  warnings.warn(\"get_all_layers() has been changed to return layers in \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Neural Network with 22963518 learnable parameters\n",
      "\n",
      "## Layer information\n",
      "\n",
      "  #  name        size\n",
      "---  --------  ------\n",
      "  0  input      12288\n",
      "  1  hidden1     1500\n",
      "  2  dropout1    1500\n",
      "  3  hidden2     1500\n",
      "  4  dropout2    1500\n",
      "  5  hidden3     1500\n",
      "  6  output        18\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NeuralNet(X_tensor_type=None,\n",
       "     batch_iterator_test=<nolearn.lasagne.base.BatchIterator object at 0xac219aac>,\n",
       "     batch_iterator_train=<nolearn.lasagne.base.BatchIterator object at 0xac23f06c>,\n",
       "     custom_score=None, dropout1_p=0.5, dropout2_p=0.5,\n",
       "     hidden1_W=<lasagne.init.GlorotUniform object at 0xac239fcc>,\n",
       "     hidden1_nonlinearity=<function rectify at 0xac49e87c>,\n",
       "     hidden1_num_units=1500,\n",
       "     hidden2_W=<lasagne.init.GlorotUniform object at 0xac239d8c>,\n",
       "     hidden2_nonlinearity=<function rectify at 0xac49e87c>,\n",
       "     hidden2_num_units=1500,\n",
       "     hidden3_W=<lasagne.init.GlorotUniform object at 0xac23f02c>,\n",
       "     hidden3_nonlinearity=<function rectify at 0xac49e87c>,\n",
       "     hidden3_num_units=1500, input_shape=(None, 12288),\n",
       "     layers=[('input', <class 'lasagne.layers.input.InputLayer'>), ('hidden1', <class 'lasagne.layers.dense.DenseLayer'>), ('dropout1', <class 'lasagne.layers.noise.DropoutLayer'>), ('hidden2', <class 'lasagne.layers.dense.DenseLayer'>), ('dropout2', <class 'lasagne.layers.noise.DropoutLayer'>), ('hidden3', <class 'lasagne.layers.dense.DenseLayer'>), ('output', <class 'lasagne.layers.dense.DenseLayer'>)],\n",
       "     loss=None, max_epochs=100, more_params={},\n",
       "     objective=<function objective at 0xac21ce9c>,\n",
       "     objective_loss_function=<function categorical_crossentropy at 0xac3dc8ec>,\n",
       "     on_epoch_finished=[<__main__.EarlyStopping object at 0xac23f08c>, <nolearn.lasagne.handlers.PrintLog instance at 0xac23f0cc>],\n",
       "     on_training_finished=[],\n",
       "     on_training_started=[<nolearn.lasagne.handlers.PrintLayerInfo instance at 0xac23f0ec>],\n",
       "     output_W=<lasagne.init.GlorotUniform object at 0xac23f04c>,\n",
       "     output_nonlinearity=<function softmax at 0xac49e6bc>,\n",
       "     output_num_units=18, regression=False,\n",
       "     train_split=<nolearn.lasagne.base.TrainSplit object at 0xac219acc>,\n",
       "     update=<function adadelta at 0xac3dcf7c>, use_label_encoder=True,\n",
       "     verbose=1, y_tensor_type=TensorType(int32, vector))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_vectorized_rescaled = StandardScaler().fit_transform(X_vectorized)\n",
    "net.fit(X_vectorized_rescaled, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see how much training is faster now ! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic gradient descent and its variants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The parameter **update** describe the optimization method used, the optimization methods are available in the module **updates** of **lasagne**. \n",
    "\n",
    "All of them use the **learning_rate** parameter which you can set by using **update_learning_rate=some_value**, some of them use the **momentum** parameter, which you can set by using **update_momentum=some_value**, some methods have their own additional hyper-parameters. \n",
    "\n",
    "For a \"live\" comparison between some training methods see this link http://cs.stanford.edu/people/karpathy/convnetjs/demo/trainers.html. \n",
    "\n",
    "Some methods:\n",
    "\n",
    "- [**updates.sgd**](http://lasagne.readthedocs.org/en/latest/modules/updates.html#lasagne.updates.sgd) , it is stochastic gradient descent, use only the learning_rate as a global learning rate\n",
    "- [**updates.momentum**](http://lasagne.readthedocs.org/en/latest/modules/updates.html#lasagne.updates.momentum), use **learning_rate** and **momentum** parameters\n",
    "- [**updates.nesterov_momentum**](http://lasagne.readthedocs.org/en/latest/modules/updates.html#lasagne.updates.nesterov_momentum) , use **learning_rate** and **momentum** parameters\n",
    "- [**updates.adagrad**](http://lasagne.readthedocs.org/en/latest/modules/updates.html#lasagne.updates.adagrad) , use only the learning_rate parameter as a global learning rate but is at the same time adaptive and it computes a learning rate for each parameter, learning rate decaying is not needed it is also done automatically.\n",
    "- [**updates.adadelta**](http://lasagne.readthedocs.org/en/latest/modules/updates.html#lasagne.updates.adadelta), it is based on **adagrad** but proposes to fix some issues that **adadelta** has, the paper states that it is not very sensitive to its hyper-parameters.\n",
    "- [**updates.adam**](http://lasagne.readthedocs.org/en/latest/modules/updates.html#lasagne.updates.adam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://i.imgur.com/s25RsOr.gif])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = NeuralNet(\n",
    "    # Define the architecture here\n",
    "    layers=[\n",
    "            ('input', layers.InputLayer), \n",
    "            ('hidden1', layers.DenseLayer),\n",
    "            ('output', layers.DenseLayer),\n",
    "    ],\n",
    "    # Layers parameters:\n",
    "    input_shape=(None, X_vectorized.shape[1]), # Number of input features\n",
    "    \n",
    "    hidden1_num_units=100,  # number of units in 1st hidden layer\n",
    "    hidden1_nonlinearity=nonlinearities.tanh,\n",
    "    hidden1_W=init.Uniform((-0.01, 0.01)),\n",
    "    \n",
    "    output_num_units=18,  # 18 classes    \n",
    "    output_W=init.Uniform((-0.01, 0.01)),\n",
    "    output_nonlinearity=nonlinearities.softmax,\n",
    "\n",
    "    # Optimization method:\n",
    "    update=updates.adadelta, # The optimization algorithm is Adadelta\n",
    "    batch_iterator_train=BatchIterator(batch_size=100), # mini-batch size\n",
    "    \n",
    "    use_label_encoder=True, # Converts labels of any kind to integers\n",
    "    max_epochs=30,  # we want to train this many epochs\n",
    "    verbose=1, # To monitor training at each epoch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "net.fit(X_vectorized_rescaled, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my experience adadelta/adagrad are good to start with, they are usually behaving well, but as many\n",
    "authors say, carefeully tuning classical stochastic gradient descent with momentum  works\n",
    "better in general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the activation functions are available in the module **nonlinearities** of **lasagne**.\n",
    "\n",
    "You can set the activation function for a given layer by **layername_nonlinearity=activation_function**\n",
    "\n",
    "Complete list : http://lasagne.readthedocs.org/en/latest/modules/nonlinearities.html\n",
    "\n",
    "- **sigmoid** : not recommanded, except for few cases, it may be useful as output non-linearities for auto-encoders with binary inputs.\n",
    "- **tanh** : rescaled version of sigmoid which is centered around zero, better than sigmoid.\n",
    "- **rectify** : Rectified linear units : $max(0, x)$, most widely used right now because they have shown to speed up training and they work well with big models. Their problem is that they are known to \"kill\" a large number of units (at some some units get always value of zero regardless of inputs, and thus will prevent the weights connected to these units to not be updated).\n",
    "- **LeakyRectify** : $max(\\alpha x, x)$ where $\\alpha > 0$ is a hyper-parameter, it is meant to reduce the dead units problem occuring with rectified linear units.\n",
    "* **leaky_rectify** : **LeakyRectify** with $\\alpha=0.01$\n",
    "* **softmax** : Used in output layers for multi-class classification problems\n",
    "\n",
    "Note that the default activation function is **rectify**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 9))\n",
    "a = np.arange(-10, 10, 0.01)\n",
    "plt.subplot(2, 2, 1)\n",
    "s = 1./(1 + np.exp(-a))\n",
    "plt.plot(a, s, color='b', label=\"f\")\n",
    "plt.plot(a, s * (1 - s), color='r', label=\"f'\")\n",
    "plt.legend()\n",
    "plt.title(\"sigmoid activation function\")\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(a, np.tanh(a), color='b', label=\"f\")\n",
    "plt.plot(a, 1 - np.tanh(a)**2, color='r', label=\"f'\")\n",
    "plt.legend()\n",
    "plt.title(\"tanh activation function\")\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(a, np.maximum(0, a), c='b', label=\"f\")\n",
    "plt.plot(a, a>0, c='r', label=\"f'\")\n",
    "plt.legend()\n",
    "plt.title(\"rectify activation function\")\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(a, np.maximum(0.01*a, a), c='b', label=\"f\")\n",
    "plt.plot(a, (a>0)*1 + (a<0)*0.01, c='r', label=\"f'\")\n",
    "plt.legend()\n",
    "plt.title(\"leaky_rectify activation function\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also implement yymour own activation function, you just create a function which takes an input x and transforms it accordingly, and then you assign this function to **layername_nonlinearity**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weight initialization is very important as it will determine the behavior of training, with a bad initialization learning will not behave correctly. \n",
    "\n",
    "Initialization schemes are available in the model **init** of **lasagne**, they can be set by **layername_W=InitializationProcedureName(params)** for weights and **layername_b=InitializationProcedureName(params)** for biases (default to 0).\n",
    "\n",
    "Weight initialization should be chosen such that it does not kill the gradients. For instance if we use **tanh** and the input of **tanh** is a linear combination between units in previous layer and weights and there are a lot of units , the scale of the input of the **tanh** will be big, so after applying **tanh** it will be saturated. So one would rescale the weights proportionally to the number of units.\n",
    "\n",
    "This first kind of initilization is called [**He**](http://lasagne.readthedocs.org/en/latest/modules/init.html#lasagne.init.He), it comes with a uniform [**HeUniform**](http://lasagne.readthedocs.org/en/latest/modules/init.html#lasagne.init.HeUniform) and a normal [**HeNormal**](http://lasagne.readthedocs.org/en/latest/modules/init.html#lasagne.init.HeNormal) version.\n",
    "\n",
    "The most popular is [**GlorotUniform**](http://lasagne.readthedocs.org/en/latest/modules/init.html#lasagne.init.GlorotUniform) which works\n",
    "usually well. For a given weight matrix, the proposed range by this scheme is not only based on the number \n",
    "of input units but also on the number output units : $$W^{(l)} \\sim Unif[-\\frac{\\sqrt{6}}{-\\sqrt{n^{(l)}+n^{(l+1)}}}, \\frac{\\sqrt{6}}{-\\sqrt{n^{(l)}+n^{(l+1)}}}]$$ where $n^{(l)}$ are the number units in layer ${(l)}$. It has also a normal distributed version with the same standard deviation than the uniform distributed version [**GlorotNormal**](http://lasagne.readthedocs.org/en/latest/modules/init.html#lasagne.init.GlorotNormal).\n",
    "\n",
    "Another scheme worth to try is [**Orthogonal**](http://lasagne.readthedocs.org/en/latest/modules/init.html#lasagne.init.GlorotUniform) initialization, which initializes weight matrices by a random semi-orthogonal matrix.\n",
    "\n",
    "**Remember** to use **gain='relu'** if you use **rectified linear units** in any of these initialization procedures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = NeuralNet(\n",
    "    # Define the architecture here\n",
    "    layers=[\n",
    "            ('input', layers.InputLayer), \n",
    "            ('hidden1', layers.DenseLayer),\n",
    "            ('hidden2', layers.DenseLayer),\n",
    "            ('hidden3', layers.DenseLayer),\n",
    "            ('output', layers.DenseLayer),\n",
    "    ],\n",
    "    # Layers parameters:\n",
    "    input_shape=(None, X_vectorized.shape[1]), # Number of input features\n",
    "    \n",
    "    hidden1_num_units=500,  # number of units in 1st hidden layer\n",
    "    hidden1_nonlinearity=nonlinearities.rectify,\n",
    "    hidden1_W=init.GlorotUniform(gain='relu'),\n",
    "    \n",
    "    hidden2_num_units=500,  # number of units in 2nd hidden layer\n",
    "    hidden2_nonlinearity=nonlinearities.rectify,\n",
    "    hidden2_W=init.GlorotUniform(gain='relu'),\n",
    "    \n",
    "    hidden3_num_units=500,  # number of units in 3rd hidden layer\n",
    "    hidden3_nonlinearity=nonlinearities.rectify,\n",
    "    hidden3_W=init.GlorotUniform(gain='relu'),\n",
    "\n",
    "    output_num_units=18,  # 18 classes    \n",
    "    output_W=init.GlorotUniform(),\n",
    "    output_nonlinearity=nonlinearities.softmax,\n",
    "\n",
    "    # Optimization method:\n",
    "    update=updates.adadelta, # The optimization algorithm is Adadelta\n",
    "    batch_iterator_train=BatchIterator(batch_size=100), # mini-batch size\n",
    "    \n",
    "    use_label_encoder=True, # Converts labels of any kind to integers\n",
    "    max_epochs=15,  # we want to train this many epochs\n",
    "    verbose=1, # To monitor training at each epoch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Neural Network with 6654518 learnable parameters\n",
      "\n",
      "## Layer information\n",
      "\n",
      "  #  name       size\n",
      "---  -------  ------\n",
      "  0  input     12288\n",
      "  1  hidden1     500\n",
      "  2  hidden2     500\n",
      "  3  hidden3     500\n",
      "  4  output       18\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NeuralNet(X_tensor_type=None,\n",
       "     batch_iterator_test=<nolearn.lasagne.base.BatchIterator object at 0xac219aac>,\n",
       "     batch_iterator_train=<nolearn.lasagne.base.BatchIterator object at 0xa9f6a96c>,\n",
       "     custom_score=None,\n",
       "     hidden1_W=<lasagne.init.GlorotUniform object at 0xa9f6a8ec>,\n",
       "     hidden1_nonlinearity=<function rectify at 0xac49e87c>,\n",
       "     hidden1_num_units=500,\n",
       "     hidden2_W=<lasagne.init.GlorotUniform object at 0xa9f6a90c>,\n",
       "     hidden2_nonlinearity=<function rectify at 0xac49e87c>,\n",
       "     hidden2_num_units=500,\n",
       "     hidden3_W=<lasagne.init.GlorotUniform object at 0xa9f6a92c>,\n",
       "     hidden3_nonlinearity=<function rectify at 0xac49e87c>,\n",
       "     hidden3_num_units=500, input_shape=(None, 12288),\n",
       "     layers=[('input', <class 'lasagne.layers.input.InputLayer'>), ('hidden1', <class 'lasagne.layers.dense.DenseLayer'>), ('hidden2', <class 'lasagne.layers.dense.DenseLayer'>), ('hidden3', <class 'lasagne.layers.dense.DenseLayer'>), ('output', <class 'lasagne.layers.dense.DenseLayer'>)],\n",
       "     loss=None, max_epochs=15, more_params={},\n",
       "     objective=<function objective at 0xac21ce9c>,\n",
       "     objective_loss_function=<function categorical_crossentropy at 0xac3dc8ec>,\n",
       "     on_epoch_finished=[<nolearn.lasagne.handlers.PrintLog instance at 0xa9f6a9ac>],\n",
       "     on_training_finished=[],\n",
       "     on_training_started=[<nolearn.lasagne.handlers.PrintLayerInfo instance at 0xa9f6a9cc>],\n",
       "     output_W=<lasagne.init.GlorotUniform object at 0xa9f6a94c>,\n",
       "     output_nonlinearity=<function softmax at 0xac49e6bc>,\n",
       "     output_num_units=18, regression=False,\n",
       "     train_split=<nolearn.lasagne.base.TrainSplit object at 0xac219acc>,\n",
       "     update=<function adadelta at 0xac3dcf7c>, use_label_encoder=True,\n",
       "     verbose=1, y_tensor_type=TensorType(int32, vector))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.fit(X_vectorized_rescaled, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have only saw about how to optimize neural networks to fit  training data, so we only dealt with **underfitting** problems occuring with neural networks. But the goal of machine learning is not to fit training data, the goal is to generalize to unseen inputs and thus to have low error on test data. \n",
    "\n",
    "As far as we fix the underfitting problem and train our neural network nicely, we can quickly observe overfitting, observed clearly by learning curves :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_loss = np.array([i[\"train_accuracy\"] for i in net.train_history_])\n",
    "valid_loss = np.array([i[\"valid_accuracy\"] for i in net.train_history_])\n",
    "plt.plot(train_loss, c=\"b\", label=\"train accuracy\", linewidth=3)\n",
    "plt.plot(valid_loss, c=\"g\", label=\"valid accuracy\", linewidth=3)\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best way to regularize is to get more data ! \n",
    "\n",
    "Unfortunately it is not always possible to gather new data \n",
    "(specially labeled ones) because it is costly. One way to cheaply gather new data is to exploit some properties\n",
    "of the data distribution to generate artificially new data. For instance, if we have images, a cheap way to generate\n",
    "data is to apply affine transformations on the images like **scaling**, **rotations** and \n",
    "**translations**. But we will go back to this later.\n",
    "\n",
    "We will first talk about classical regularization techniques used in neural networks and see how to use them\n",
    "in **Nolearn**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to regularize a neural network is to constrain its capacity. Two forms of weight decay penalties are **L1** and **L2**, **L1** penalizes the absolute value of the weights while **L2** penalizes the squared value of the weights. **L1** tends to give sparse values to the weights (that is, exact zero values to a subset of weights).\n",
    "\n",
    "For both **L1** and **L2** there is a hyper-parameter $\\lambda$, called the **regularization coeficient**, it is controlling how much you give importance to regularization compared with fitting the training data (the objective function), it must be a positive real number and it is usually very small, e.g 0.0001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nolearn.lasagne.base import objective\n",
    "from lasagne.objectives import aggregate\n",
    "from lasagne.regularization import regularize_layer_params, l2, l1\n",
    "\n",
    "lambda_regularization = 0.08\n",
    "\n",
    "def objective_with_L2(layers,\n",
    "                      loss_function,\n",
    "                      target,\n",
    "                      aggregate=aggregate,\n",
    "                      deterministic=False,\n",
    "                      get_output_kw=None):\n",
    "    reg = regularize_layer_params([layers[\"hidden1\"], layers[\"hidden2\"], layers[\"hidden3\"]], l2)\n",
    "    loss = objective(layers, loss_function, target, aggregate, deterministic, get_output_kw)\n",
    "    \n",
    "    if deterministic is False:\n",
    "        return loss + reg * lambda_regularization\n",
    "    else:\n",
    "        return loss\n",
    "\n",
    "net = NeuralNet(\n",
    "    # Define the architecture here\n",
    "    layers=[\n",
    "            ('input', layers.InputLayer), \n",
    "            ('hidden1', layers.DenseLayer),\n",
    "            ('hidden2', layers.DenseLayer),\n",
    "            ('hidden3', layers.DenseLayer),\n",
    "            ('output', layers.DenseLayer),\n",
    "    ],\n",
    "    # Layers parameters:\n",
    "    input_shape=(None, X_vectorized.shape[1]), # Number of input features\n",
    "    \n",
    "    hidden1_num_units=500,  # number of units in 1st hidden layer\n",
    "    hidden1_nonlinearity=nonlinearities.rectify,\n",
    "    hidden1_W=init.GlorotUniform(gain='relu'),\n",
    "    \n",
    "    hidden2_num_units=500,  # number of units in 2nd hidden layer\n",
    "    hidden2_nonlinearity=nonlinearities.rectify,\n",
    "    hidden2_W=init.GlorotUniform(gain='relu'),\n",
    "    \n",
    "    hidden3_num_units=500,  # number of units in 3rd hidden layer\n",
    "    hidden3_nonlinearity=nonlinearities.rectify,\n",
    "    hidden3_W=init.GlorotUniform(gain='relu'),\n",
    "\n",
    "    output_num_units=18,  # 18 classes    \n",
    "    output_W=init.GlorotUniform(),\n",
    "    output_nonlinearity=nonlinearities.softmax,\n",
    "    \n",
    "    # objective function\n",
    "    objective=objective_with_L2,\n",
    "    # Optimization method:\n",
    "    update=updates.adadelta, # The optimization algorithm is Adadelta\n",
    "    batch_iterator_train=BatchIterator(batch_size=100), # mini-batch size\n",
    "    \n",
    "    use_label_encoder=True, # Converts labels of any kind to integers\n",
    "    max_epochs=100,  # we want to train this many epochs\n",
    "    verbose=1, # To monitor training at each epoch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net.fit(X_vectorized_rescaled, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_loss = np.array([i[\"train_accuracy\"] for i in net.train_history_])\n",
    "valid_loss = np.array([i[\"valid_accuracy\"] for i in net.train_history_])\n",
    "plt.plot(train_loss, c=\"b\", label=\"train accuracy\", linewidth=3)\n",
    "plt.plot(valid_loss, c=\"g\", label=\"valid accuracy\", linewidth=3)\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we are not overfitting, but by restricting the capacity of our neural network a lot we are now underfitting ! \n",
    "\n",
    "Another cheap ad simple way to regularize is to use **dropout**. **Dropout** as a way of preventing\n",
    "units of co-adapting by randomly **dropping** units in each layer, that is setting their values to 0\n",
    "with a certain probability, with $p=0.5$ usually. \n",
    "By applying **dropout**  for a given layer, we force units to be useful as themselves to the next\n",
    "layer units. **Dropout** can also be seen as a way to create an ensemble of neural networks with\n",
    "shared parameters. Thus, at test time, to approximate averaging of this ensemble with multiply\n",
    "the units by **p**.\n",
    "\n",
    "**Dropout** are a kind of layers in **lasagne**, they take the previous layer, randomly zero-out their\n",
    "units with the given probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = NeuralNet(\n",
    "    # Define the architecture here\n",
    "    layers=[\n",
    "            ('input', layers.InputLayer), \n",
    "            ('hidden1', layers.DenseLayer),\n",
    "            ('dropout1', layers.DropoutLayer),\n",
    "            ('hidden2', layers.DenseLayer),\n",
    "            ('dropout2', layers.DropoutLayer),\n",
    "            ('hidden3', layers.DenseLayer),\n",
    "            ('output', layers.DenseLayer),\n",
    "    ],\n",
    "    # Layers parameters:\n",
    "    input_shape=(None, X_vectorized.shape[1]), # Number of input features\n",
    "    \n",
    "    hidden1_num_units=1500,  # number of units in 1st hidden layer\n",
    "    hidden1_nonlinearity=nonlinearities.rectify,\n",
    "    hidden1_W=init.GlorotUniform(gain='relu'),\n",
    "    \n",
    "    dropout1_p=0.5,\n",
    "    \n",
    "    hidden2_num_units=1500,  # number of units in 2nd hidden layer\n",
    "    hidden2_nonlinearity=nonlinearities.rectify,\n",
    "    hidden2_W=init.GlorotUniform(gain='relu'),\n",
    "    \n",
    "    dropout2_p=0.5,\n",
    "    \n",
    "    hidden3_num_units=1500,  # number of units in 3rd hidden layer\n",
    "    hidden3_nonlinearity=nonlinearities.rectify,\n",
    "    hidden3_W=init.GlorotUniform(gain='relu'),\n",
    "\n",
    "    output_num_units=18,  # 18 classes    \n",
    "    output_W=init.GlorotUniform(),\n",
    "    output_nonlinearity=nonlinearities.softmax,\n",
    "    \n",
    "    # Optimization method:\n",
    "    update=updates.adadelta, # The optimization algorithm is Adadelta\n",
    "    batch_iterator_train=BatchIterator(batch_size=100), # mini-batch size\n",
    "    \n",
    "    use_label_encoder=True, # Converts labels of any kind to integers\n",
    "    max_epochs=100,  # we want to train this many epochs\n",
    "    verbose=1, # To monitor training at each epoch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net.fit(X_vectorized_rescaled, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another common way of regularizing is to use **early stopping**. As said above, in machine learning we are interested in validation error not in training error so usually we monitor the validation error and stop when it starts increasing. Early stopping is an attempt to automatize this process. It is done by monitoring validation error and memorizing the best model so far in terms of validation error and it stops when the validation error is not decreasing after a certain number epochs determined by a **patience** parameter. The chosen model after stopping is the one with the best error on validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do that in Nolearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EarlyStopping(object):\n",
    "\n",
    "    def __init__(self, patience=100, criterion='valid_loss',\n",
    "                 criterion_smaller_is_better=True):\n",
    "        self.patience = patience\n",
    "        if criterion_smaller_is_better is True:\n",
    "            self.best_valid = np.inf\n",
    "        else:\n",
    "            self.best_valid = -np.inf\n",
    "        self.best_valid_epoch = 0\n",
    "        self.best_weights = None\n",
    "        self.criterion = criterion\n",
    "        self.criterion_smaller_is_better = criterion_smaller_is_better\n",
    "\n",
    "    def __call__(self, nn, train_history):\n",
    "        current_valid = train_history[-1][self.criterion]\n",
    "        current_epoch = train_history[-1]['epoch']\n",
    "        if self.criterion_smaller_is_better:\n",
    "            cond = current_valid < self.best_valid\n",
    "        else:\n",
    "            cond = current_valid > self.best_valid\n",
    "        if cond:\n",
    "            self.best_valid = current_valid\n",
    "            self.best_valid_epoch = current_epoch\n",
    "            self.best_weights = nn.get_all_params_values()\n",
    "        elif self.best_valid_epoch + self.patience < current_epoch:\n",
    "            if nn.verbose:\n",
    "                print(\"Early stopping.\")\n",
    "                print(\"Best {:s} was {:.6f} at epoch {}.\".format(\n",
    "                    self.criterion, self.best_valid, self.best_valid_epoch))\n",
    "            nn.load_weights_from(self.best_weights)\n",
    "            if nn.verbose:\n",
    "                print(\"Weights set.\")\n",
    "            raise StopIteration()\n",
    "\n",
    "    def load_best_weights(self, nn, train_history):\n",
    "        nn.load_weights_from(self.best_weights)\n",
    "        \n",
    "net = NeuralNet(\n",
    "    # Define the architecture here\n",
    "    layers=[\n",
    "            ('input', layers.InputLayer), \n",
    "            ('hidden1', layers.DenseLayer),\n",
    "            ('dropout1', layers.DropoutLayer),\n",
    "            ('hidden2', layers.DenseLayer),\n",
    "            ('dropout2', layers.DropoutLayer),\n",
    "            ('hidden3', layers.DenseLayer),\n",
    "            ('output', layers.DenseLayer),\n",
    "    ],\n",
    "    # Layers parameters:\n",
    "    input_shape=(None, X_vectorized.shape[1]), # Number of input features\n",
    "    \n",
    "    hidden1_num_units=1500,  # number of units in 1st hidden layer\n",
    "    hidden1_nonlinearity=nonlinearities.rectify,\n",
    "    hidden1_W=init.GlorotUniform(gain='relu'),\n",
    "    \n",
    "    dropout1_p=0.5,\n",
    "    \n",
    "    hidden2_num_units=1500,  # number of units in 2nd hidden layer\n",
    "    hidden2_nonlinearity=nonlinearities.rectify,\n",
    "    hidden2_W=init.GlorotUniform(gain='relu'),\n",
    "    \n",
    "    dropout2_p=0.5,\n",
    "    \n",
    "    hidden3_num_units=1500,  # number of units in 3rd hidden layer\n",
    "    hidden3_nonlinearity=nonlinearities.rectify,\n",
    "    hidden3_W=init.GlorotUniform(gain='relu'),\n",
    "\n",
    "    output_num_units=18,  # 18 classes    \n",
    "    output_W=init.GlorotUniform(),\n",
    "    output_nonlinearity=nonlinearities.softmax,\n",
    "    \n",
    "    # Optimization method:\n",
    "    update=updates.adadelta, # The optimization algorithm is Adadelta\n",
    "    batch_iterator_train=BatchIterator(batch_size=100), # mini-batch size\n",
    "    \n",
    "    use_label_encoder=True, # Converts labels of any kind to integers\n",
    "    max_epochs=100,  # we want to train this many epochs\n",
    "    verbose=1, # To monitor training at each epoch\n",
    "    \n",
    "    # handlers\n",
    "    on_epoch_finished = [EarlyStopping(patience=10, criterion='valid_accuracy', \n",
    "                                       criterion_smaller_is_better=False)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Neural Network with 22963518 learnable parameters\n",
      "\n",
      "## Layer information\n",
      "\n",
      "  #  name        size\n",
      "---  --------  ------\n",
      "  0  input      12288\n",
      "  1  hidden1     1500\n",
      "  2  dropout1    1500\n",
      "  3  hidden2     1500\n",
      "  4  dropout2    1500\n",
      "  5  hidden3     1500\n",
      "  6  output        18\n",
      "\n",
      "0.0660377358491\n",
      "  epoch    train loss    valid loss    train/val    train_acc    valid acc  dur\n",
      "-------  ------------  ------------  -----------  -----------  -----------  -----\n",
      "      1     \u001b[36m119.17730\u001b[0m       \u001b[32m4.47933\u001b[0m     26.60605      0.11426      0.06604  7.68s\n",
      "0.254716981132\n",
      "      2       \u001b[36m6.35879\u001b[0m       \u001b[32m3.25390\u001b[0m      1.95421      0.14995      0.25472  4.63s\n",
      "      3       \u001b[36m4.40686\u001b[0m       3.73430      1.18010      0.22995      0.24528  4.33s\n",
      "      4       \u001b[36m3.79025\u001b[0m       4.21212      0.89984      0.27899      0.21698  5.63s\n",
      "      5       4.03861       \u001b[32m2.97540\u001b[0m      1.35734      0.32809      0.23585  5.28s\n",
      "0.27358490566\n",
      "      6       \u001b[36m2.97587\u001b[0m       \u001b[32m2.80355\u001b[0m      1.06147      0.35527      0.27358  4.08s\n",
      "0.301886792453\n",
      "      7       \u001b[36m2.86449\u001b[0m       2.87950      0.99479      0.39606      0.30189  4.50s\n",
      "      8       3.02307       \u001b[32m2.68034\u001b[0m      1.12787      0.38138      0.27358  4.15s\n",
      "      9       \u001b[36m2.75823\u001b[0m       2.98971      0.92257      0.46484      0.28302  6.61s\n",
      "     10       \u001b[36m2.46124\u001b[0m       2.76815      0.88913      0.46734      0.28302  4.04s\n",
      "     11       2.52483       2.80777      0.89923      0.51814      0.21698  4.09s\n",
      "     12       2.66612       2.83109      0.94173      0.52718      0.29245  4.05s\n",
      "     13       2.64471       2.84567      0.92938      0.55234      0.30189  4.15s\n",
      "     14       \u001b[36m2.36183\u001b[0m       4.42333      0.53395      0.55798      0.27358  4.09s\n",
      "     15       2.77649       2.76859      1.00286      0.55096      0.26415  4.32s\n",
      "     16       2.82574       2.70683      1.04393      0.53968      0.25472  4.33s\n",
      "     17       \u001b[36m2.31492\u001b[0m       2.81330      0.82285      0.55266      0.21698  4.57s\n",
      "Early stopping.\n",
      "Best valid_accuracy was 0.301887 at epoch 7.\n",
      "Loaded parameters to layer 'hidden1' (shape 12288x1500).\n",
      "Loaded parameters to layer 'hidden1' (shape 1500).\n",
      "Loaded parameters to layer 'hidden2' (shape 1500x1500).\n",
      "Loaded parameters to layer 'hidden2' (shape 1500).\n",
      "Loaded parameters to layer 'hidden3' (shape 1500x1500).\n",
      "Loaded parameters to layer 'hidden3' (shape 1500).\n",
      "Loaded parameters to layer 'output' (shape 1500x18).\n",
      "Loaded parameters to layer 'output' (shape 18).\n",
      "Weights set.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NeuralNet(X_tensor_type=None,\n",
       "     batch_iterator_test=<nolearn.lasagne.base.BatchIterator object at 0xac219aac>,\n",
       "     batch_iterator_train=<nolearn.lasagne.base.BatchIterator object at 0x897699ec>,\n",
       "     custom_score=None, dropout1_p=0.5, dropout2_p=0.5,\n",
       "     hidden1_W=<lasagne.init.GlorotUniform object at 0x89a9df6c>,\n",
       "     hidden1_nonlinearity=<function rectify at 0xac49e87c>,\n",
       "     hidden1_num_units=1500,\n",
       "     hidden2_W=<lasagne.init.GlorotUniform object at 0x89852d2c>,\n",
       "     hidden2_nonlinearity=<function rectify at 0xac49e87c>,\n",
       "     hidden2_num_units=1500,\n",
       "     hidden3_W=<lasagne.init.GlorotUniform object at 0x8e4b852c>,\n",
       "     hidden3_nonlinearity=<function rectify at 0xac49e87c>,\n",
       "     hidden3_num_units=1500, input_shape=(None, 12288),\n",
       "     layers=[('input', <class 'lasagne.layers.input.InputLayer'>), ('hidden1', <class 'lasagne.layers.dense.DenseLayer'>), ('dropout1', <class 'lasagne.layers.noise.DropoutLayer'>), ('hidden2', <class 'lasagne.layers.dense.DenseLayer'>), ('dropout2', <class 'lasagne.layers.noise.DropoutLayer'>), ('hidden3', <class 'lasagne.layers.dense.DenseLayer'>), ('output', <class 'lasagne.layers.dense.DenseLayer'>)],\n",
       "     loss=None, max_epochs=100, more_params={},\n",
       "     objective=<function objective at 0xac21ce9c>,\n",
       "     objective_loss_function=<function categorical_crossentropy at 0xac3dc8ec>,\n",
       "     on_epoch_finished=[<__main__.EarlyStopping object at 0x89769a6c>, <nolearn.lasagne.handlers.PrintLog instance at 0x8967c0cc>],\n",
       "     on_training_finished=[],\n",
       "     on_training_started=[<nolearn.lasagne.handlers.PrintLayerInfo instance at 0x8967c0ec>],\n",
       "     output_W=<lasagne.init.GlorotUniform object at 0x8976930c>,\n",
       "     output_nonlinearity=<function softmax at 0xac49e6bc>,\n",
       "     output_num_units=18, regression=False,\n",
       "     train_split=<nolearn.lasagne.base.TrainSplit object at 0xac219acc>,\n",
       "     update=<function adadelta at 0xac3dcf7c>, use_label_encoder=True,\n",
       "     verbose=1, y_tensor_type=TensorType(int32, vector))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.fit(X_vectorized_rescaled, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
