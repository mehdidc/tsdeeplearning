{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Neural networks, recommandations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we introduce some heuristics to select hyper-parameters and\n",
    "to debug a neural net."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning rate is a very important hyper-parameters, neural networks are very sensitive\n",
    "to its value, so you should tune it well because it affects optimization a lot.\n",
    "\n",
    "Try values in logarithmic scale : $$10^{(-a)}$$ where a is an integer, the exact\n",
    "value does not matter a lot, what matters is the order of magnitude.\n",
    "\n",
    "Estimating learning rate (from stochastic gradient descent tricks, Leon Bottou):\n",
    "\n",
    "If you are using stochastic gradient descent (SGD), you can estimate learning rate from a subset of data and then either the same or a slightly smaller learning rate on the full data.\n",
    "\n",
    "The validation performance should plateau after a number of epochs comparable to to the number\n",
    "of epochs needed to reach this point on the small training set.\n",
    "\n",
    "You may also try to decay your learning rate following the rule $\\lambda_t = \\frac{\\lambda_0}{(1 + \\lambda_0\\lambda_t)}$, estimate $\\lambda_0$ from a subset of training set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Normally a small batch_size is better because it performs more updates per epoch (e.g with batch_size=1, there is an update for **each example**), but if you want to benefit from parallelism you would want to use bigger batch_size to speed up computations, but of course if batch_size is very big you end up doing less updates, so it is a trade-off between number of updates per epoch and computation speed, one would like an intermediate value of **batch_size**.\n",
    "\n",
    "You can choose it independently of other hyper-parameters, plot train and validation accuracy vs\n",
    "time elapsed for different batch sizes and choose the one that converges faster.\n",
    "\n",
    "Source : http://neuralnetworksanddeeplearning.com/chap3.html#how_to_choose_a_neural_network's_hyper-parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you use mini-batch gradient descent, Randomly shuffle the dataset in each epoch, it can speed up convergence because it  emulates more closely the requirements of SGD (which requires that examples are sampled randomly from the distribution)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use early stopping to decide when to stop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the magnitude of the weights and their updates (at beginning and during training), typically the updates must be have\n",
    "a smaller order of magnitude but should not very small (and not very big), for instance 10% or 1% or the magnitude of the weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one way to deal with regularization is to penalize large weights (**weight decay)**, two forms of weight decay penalties are L1 and L2, L1 penalizes the absolute value of the weights while L2 penalizes the squared value of the weights. L1 tends to give sparse values to the weights.\n",
    "\n",
    "The regularization coeficient is controlling how much you give importance to regularization compared with fitting the training data (the objective function), it must be a positive real number and it is usually very small, e.g 0.0001."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Favour random search over grid search : http://scikit-learn.org/stable/auto_examples/model_selection/randomized_search.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, it is always better to use regularization to control overfitting instead of lowering the number of units."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
