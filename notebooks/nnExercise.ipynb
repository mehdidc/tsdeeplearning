{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.python.org/dev/peps/pep-0008#introduction<BR>\n",
    "http://scikit-learn.org/<BR>\n",
    "http://pandas.pydata.org/<BR>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from time import sleep\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch the data and load it in pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Elevation</th>\n",
       "      <th>Aspect</th>\n",
       "      <th>Slope</th>\n",
       "      <th>Horizontal_Distance_To_Hydrology</th>\n",
       "      <th>Vertical_Distance_To_Hydrology</th>\n",
       "      <th>Horizontal_Distance_To_Roadways</th>\n",
       "      <th>Hillshade_9am</th>\n",
       "      <th>Hillshade_Noon</th>\n",
       "      <th>Hillshade_3pm</th>\n",
       "      <th>...</th>\n",
       "      <th>Soil_Type32</th>\n",
       "      <th>Soil_Type33</th>\n",
       "      <th>Soil_Type34</th>\n",
       "      <th>Soil_Type35</th>\n",
       "      <th>Soil_Type36</th>\n",
       "      <th>Soil_Type37</th>\n",
       "      <th>Soil_Type38</th>\n",
       "      <th>Soil_Type39</th>\n",
       "      <th>Soil_Type40</th>\n",
       "      <th>Cover_Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2596</td>\n",
       "      <td>51</td>\n",
       "      <td>3</td>\n",
       "      <td>258</td>\n",
       "      <td>0</td>\n",
       "      <td>510</td>\n",
       "      <td>221</td>\n",
       "      <td>232</td>\n",
       "      <td>148</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2590</td>\n",
       "      <td>56</td>\n",
       "      <td>2</td>\n",
       "      <td>212</td>\n",
       "      <td>-6</td>\n",
       "      <td>390</td>\n",
       "      <td>220</td>\n",
       "      <td>235</td>\n",
       "      <td>151</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2804</td>\n",
       "      <td>139</td>\n",
       "      <td>9</td>\n",
       "      <td>268</td>\n",
       "      <td>65</td>\n",
       "      <td>3180</td>\n",
       "      <td>234</td>\n",
       "      <td>238</td>\n",
       "      <td>135</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2785</td>\n",
       "      <td>155</td>\n",
       "      <td>18</td>\n",
       "      <td>242</td>\n",
       "      <td>118</td>\n",
       "      <td>3090</td>\n",
       "      <td>238</td>\n",
       "      <td>238</td>\n",
       "      <td>122</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2595</td>\n",
       "      <td>45</td>\n",
       "      <td>2</td>\n",
       "      <td>153</td>\n",
       "      <td>-1</td>\n",
       "      <td>391</td>\n",
       "      <td>220</td>\n",
       "      <td>234</td>\n",
       "      <td>150</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  Elevation  Aspect  Slope  Horizontal_Distance_To_Hydrology  \\\n",
       "0   1       2596      51      3                               258   \n",
       "1   2       2590      56      2                               212   \n",
       "2   3       2804     139      9                               268   \n",
       "3   4       2785     155     18                               242   \n",
       "4   5       2595      45      2                               153   \n",
       "\n",
       "   Vertical_Distance_To_Hydrology  Horizontal_Distance_To_Roadways  \\\n",
       "0                               0                              510   \n",
       "1                              -6                              390   \n",
       "2                              65                             3180   \n",
       "3                             118                             3090   \n",
       "4                              -1                              391   \n",
       "\n",
       "   Hillshade_9am  Hillshade_Noon  Hillshade_3pm     ...      Soil_Type32  \\\n",
       "0            221             232            148     ...                0   \n",
       "1            220             235            151     ...                0   \n",
       "2            234             238            135     ...                0   \n",
       "3            238             238            122     ...                0   \n",
       "4            220             234            150     ...                0   \n",
       "\n",
       "   Soil_Type33  Soil_Type34  Soil_Type35  Soil_Type36  Soil_Type37  \\\n",
       "0            0            0            0            0            0   \n",
       "1            0            0            0            0            0   \n",
       "2            0            0            0            0            0   \n",
       "3            0            0            0            0            0   \n",
       "4            0            0            0            0            0   \n",
       "\n",
       "   Soil_Type38  Soil_Type39  Soil_Type40  Cover_Type  \n",
       "0            0            0            0           5  \n",
       "1            0            0            0           5  \n",
       "2            0            0            0           2  \n",
       "3            0            0            0           2  \n",
       "4            0            0            0           5  \n",
       "\n",
       "[5 rows x 56 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare input to scikit and train and test cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "binary_data = data[np.logical_or(data['Cover_Type'] == 1,data['Cover_Type'] == 2)] # two-class classification set\n",
    "X = binary_data.drop('Cover_Type', axis=1).values\n",
    "y = binary_data['Cover_Type'].values\n",
    "y = 2 * y - 3 # converting labels from [1,2] to [-1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied (use --upgrade to upgrade): tabulate in /Users/kegl/anaconda/lib/python2.7/site-packages\n",
      "Collecting git+https://github.com/mehdidc/Lasagne\n",
      "  Cloning https://github.com/mehdidc/Lasagne to /var/folders/x0/wg4j0l5d1fj8hx0zqpjrvmpr0000gn/T/pip-jxmshl-build\n",
      "  Requirement already satisfied (use --upgrade to upgrade): Lasagne==0.1.dev0 from git+https://github.com/mehdidc/Lasagne in /Users/kegl/anaconda/lib/python2.7/site-packages\n",
      "Requirement already satisfied (use --upgrade to upgrade): numpy in /Users/kegl/anaconda/lib/python2.7/site-packages (from Lasagne==0.1.dev0)\n",
      "Requirement already satisfied (use --upgrade to upgrade): Theano in /Users/kegl/anaconda/lib/python2.7/site-packages (from Lasagne==0.1.dev0)\n",
      "Requirement already satisfied (use --upgrade to upgrade): scipy>=0.11 in /Users/kegl/anaconda/lib/python2.7/site-packages (from Theano->Lasagne==0.1.dev0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tabulate\n",
    "!pip install git+https://github.com/mehdidc/Lasagne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lasagne.easy import SimpleNeuralNet\n",
    "from lasagne.easy import get_stat\n",
    "from sklearn.preprocessing import StandardScaler, Imputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import theano\n",
    "import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('neuralnet', SimpleNeuralNet(nb_hidden_list=[100, 100],\n",
    "                                      max_nb_epochs=100,\n",
    "                                      batch_size=10,\n",
    "                                      learning_rate=0.9,\n",
    "                                      #L1_factor=0.001,\n",
    "                                      verbose = 1,\n",
    "                                      ))\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "      0          0.207502      0.524621          0.762731\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "      1          0.185381      0.465653          0.778935\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "      2          0.186232      0.446761          0.789931\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "      3          0.185624      0.430659          0.796586\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "      4          0.186735       0.41612          0.801794\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "      5          0.186639      0.401859          0.809606\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "      6          0.190898      0.387653          0.817419\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "      7          0.192981      0.374035          0.824653\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "      8          0.192929      0.359619          0.831597\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "      9          0.192949      0.343616          0.840856\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     10          0.192193      0.329707          0.844907\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     11          0.191363      0.314738          0.854745\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     12          0.188769       0.29995          0.863715\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     13          0.187962      0.286671          0.869502\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     14           0.18417      0.272246           0.87934\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     15          0.183032      0.259201          0.883681\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     16          0.183659      0.246516          0.890336\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     17          0.179349      0.236233          0.890625\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     18          0.179577       0.22378          0.897859\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     19           0.17375      0.212714          0.904803\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     20          0.175985      0.204761          0.899595\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     21          0.176294       0.19467          0.908275\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     22           0.17319      0.184999          0.907986\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     23          0.168662      0.175273          0.915509\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     24          0.168863      0.167781           0.91956\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     25          0.161482      0.157927          0.925347\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     26          0.164671       0.15418           0.91985\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     27          0.159952      0.144104          0.921875\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     28          0.154189      0.137097          0.927373\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     29          0.173009      0.136012          0.925637\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     30          0.156905      0.127577          0.929688\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     31          0.139017      0.116924          0.929109\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     32            0.1536      0.116866          0.930556\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     33          0.167043      0.113139          0.936632\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     34          0.142153      0.104437          0.940683\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     35           0.14531      0.102785           0.94647\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     36          0.291195       0.11344          0.916667\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     37          0.255213      0.104621          0.916088\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     38          0.134545     0.0894926          0.928819\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     39          0.130449     0.0842602           0.95081\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     40          0.129222     0.0825023          0.953125\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     41          0.147182     0.0832006          0.952546\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     42           0.15645     0.0790935          0.953704\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     43          0.120198     0.0704538          0.950521\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     44          0.127614     0.0719538          0.959491\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     45          0.168175     0.0835277          0.965278\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     46          0.143147     0.0729937          0.953704\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     47          0.107955     0.0618759          0.961516\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     48           0.16626      0.065122          0.963542\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     49          0.145352     0.0675351          0.962384\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     50          0.121321     0.0594625          0.961227\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     51          0.137191     0.0618394          0.964988\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     52          0.173265     0.0699035          0.971644\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     53          0.166795     0.0624793          0.969039\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     54          0.115769     0.0531268          0.973958\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     55          0.213428     0.0624963          0.916956\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     56          0.179668     0.0678131          0.969907\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     57          0.171776     0.0571336          0.975405\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     58          0.126594     0.0445298          0.969329\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     59          0.115035      0.052012           0.95515\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     60          0.178057     0.0551421          0.971644\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     61          0.214318      0.057371          0.937211\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     62          0.078308     0.0370678          0.967303\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     63         0.0739844     0.0334837          0.971933\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     64          0.124712     0.0393627          0.972512\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     65          0.135766     0.0494103          0.979167\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     66          0.128525     0.0440845          0.976562\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     67         0.0959649     0.0362886          0.985532\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     68          0.147021     0.0432531          0.977141\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     69         0.0910565     0.0330762          0.982928\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     70         0.0862787     0.0320009          0.976852\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     71           0.10655     0.0347569            0.9864\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     72         0.0899455     0.0320697          0.980613\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     73         0.0767237     0.0267597           0.97338\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     74           0.19622     0.0421049          0.973958\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     75         0.0737693      0.026409          0.956308\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     76           0.18181     0.0328146          0.983796\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     77          0.137029     0.0352143          0.972801\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     78          0.146371     0.0367733          0.981481\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     79           0.10506     0.0269449           0.98206\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     80         0.0731306     0.0218225          0.982928\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     81         0.0975869     0.0242921          0.979745\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     82         0.0657785     0.0200917          0.984086\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     83          0.103578     0.0308339           0.97772\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     84           0.19983     0.0386192          0.989294\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     85         0.0620975     0.0177378           0.98206\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     86          0.128979     0.0238068           0.98206\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     87          0.143672     0.0345711          0.984954\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     88          0.155044     0.0301743          0.985243\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     89         0.0940816     0.0227928          0.979745\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     90         0.0760839     0.0175769          0.988715\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     91         0.0615744     0.0159336          0.987847\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     92         0.0751889     0.0206831          0.980613\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     93          0.124063     0.0232777          0.980613\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     94         0.0610331     0.0141427          0.986979\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     95          0.169964      0.027092          0.991898\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     96          0.165367     0.0230732          0.990162\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     97          0.165474      0.035268          0.959201\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     98         0.0359169     0.0100847          0.990741\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     99         0.0923492     0.0217266          0.993634\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('neuralnet', <lasagne.easy.SimpleNeuralNet object at 0x110645a10>)])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Train a neural net\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80439814814814814"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing with SimpleNeuralNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation functions\n",
    "\n",
    "the parameter **activations** describes the non-linearities used in each layer. It is either a list of strings with the same  size as nb_hidden_list, or one string and in that case the same non-linearities will be used everywhere. The default non-linearity used is the rectified linear units (ReLU) everywhere.\n",
    "\n",
    "Examples : **'relu', 'tanh', 'sigmoid'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "      0          0.188945      0.526821          0.756655\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "      1          0.179699       0.48866           0.76794\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "      2          0.174357      0.472325          0.775752\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "      3          0.171454      0.459326          0.782986\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "      4          0.169793      0.447728          0.791088\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "      5          0.168578      0.436578          0.798322\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "      6          0.167412      0.425223          0.806713\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "      7          0.166164      0.413698          0.813657\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "      8          0.164828      0.402241          0.817998\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "      9          0.163397      0.390923          0.824653\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     10          0.161878      0.379747          0.833044\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     11          0.160252      0.368686          0.838252\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     12          0.158526      0.357674          0.844329\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     13          0.156881      0.346739          0.851562\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     14          0.155503      0.335938          0.858218\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     15          0.154366      0.325298          0.862558\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     16          0.153311      0.314931          0.868056\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     17          0.152201      0.304774          0.873843\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     18           0.15097      0.294723          0.879051\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     19          0.149596      0.284754          0.882523\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     20          0.148152      0.274873          0.888021\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     21          0.146672      0.265084           0.89265\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     22          0.145105      0.255373           0.89728\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     23          0.143368      0.245764          0.903356\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     24          0.141428      0.236297          0.909722\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     25          0.139267      0.226992          0.913773\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     26          0.136848      0.217828          0.916956\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     27          0.134142      0.208765          0.918981\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     28          0.131193      0.199783          0.923611\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     29          0.128036      0.190857           0.92853\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     30          0.124711      0.182002           0.93287\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     31          0.121223      0.173235            0.9375\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     32          0.117656      0.164617          0.939236\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     33          0.114065      0.156194          0.940972\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     34          0.110452      0.147968          0.944734\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     35          0.106817       0.13997          0.946759\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     36          0.103146      0.132212           0.95081\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     37         0.0993853      0.124688          0.952257\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     38         0.0955286      0.117412          0.954861\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     39         0.0916137      0.110401          0.958623\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     40          0.087714      0.103667          0.960069\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     41         0.0838827     0.0972158          0.962095\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     42         0.0801741     0.0910648           0.96441\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     43         0.0765996     0.0852307          0.966725\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     44         0.0731681     0.0797042          0.968461\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     45         0.0699138     0.0744817          0.970486\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     46         0.0668668     0.0695702          0.972512\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     47         0.0640615     0.0649794          0.973669\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     48         0.0614883     0.0607031          0.974826\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     49         0.0591131     0.0567346          0.976852\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     50         0.0569222     0.0530657          0.978877\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     51         0.0549294     0.0496857          0.980324\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     52         0.0531579     0.0465779          0.981771\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     53         0.0516008     0.0437175          0.983796\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     54         0.0502303     0.0410778          0.985532\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     55             0.049     0.0386365          0.985822\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     56         0.0478638     0.0363763          0.986111\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     57         0.0467864     0.0342788           0.98669\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     58         0.0457506     0.0323251          0.987558\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     59         0.0447524     0.0304977          0.988137\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     60         0.0437859     0.0287857          0.988715\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     61         0.0428454     0.0271741          0.989873\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     62           0.04193     0.0256609          0.990741\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     63         0.0410538     0.0242422          0.991898\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     64          0.040235      0.022918          0.992477\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     65         0.0394885     0.0216872          0.992766\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     66         0.0387977     0.0205438          0.993345\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     67         0.0381269     0.0194812          0.993345\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     68         0.0374309     0.0184906          0.993345\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     69         0.0366884     0.0175687          0.993924\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     70         0.0358948     0.0167108          0.994502\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     71         0.0350497     0.0159107          0.995081\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     72         0.0342021     0.0151677          0.995081\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     73         0.0334027     0.0144797          0.995081\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     74         0.0327049     0.0138496          0.995081\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     75         0.0321202     0.0132707           0.99566\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     76         0.0316704     0.0127314           0.99566\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     77         0.0313563     0.0122231           0.99566\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     78         0.0310578     0.0117405          0.995949\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     79         0.0306959     0.0112768          0.995949\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     80         0.0302824     0.0108322           0.99566\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     81          0.029831     0.0104053           0.99537\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     82         0.0293643    0.00999551          0.995949\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     83         0.0289242    0.00960271          0.996238\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     84         0.0284925    0.00922615          0.996238\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     85         0.0280557    0.00886335          0.995949\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     86         0.0276358    0.00851519          0.995949\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     87         0.0272087    0.00818098          0.996238\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     88         0.0267843    0.00786146          0.996238\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     89         0.0263659    0.00755641          0.996528\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     90         0.0259453    0.00726507          0.996528\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     91         0.0255221    0.00698912          0.996528\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     92         0.0250601    0.00672439          0.996528\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     93         0.0245608    0.00646946          0.996817\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     94         0.0240063    0.00622009          0.996817\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     95         0.0233992    0.00597434          0.996817\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     96         0.0227426    0.00573371          0.996817\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     97         0.0220486    0.00550101          0.997975\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     98         0.0213189    0.00527707          0.997975\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     99         0.0205644    0.00506308          0.997975\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('neuralnet', <lasagne.easy.SimpleNeuralNet object at 0x10c04dfd0>)])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('neuralnet', SimpleNeuralNet(nb_hidden_list=[100, 100],\n",
    "                                      max_nb_epochs=100,\n",
    "                                      activations=[\"tanh\", \"tanh\"], # or just activations=\"tanh\"\n",
    "                                      batch_size=10,\n",
    "                                      learning_rate=0.5,\n",
    "                                      verbose = 1,\n",
    "                                      ))\n",
    "        ])\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization methods\n",
    "\n",
    "the parameter **optimization_method** describe the optimization method used. All of them use the **learning_rate** parameter, some of them use the **momentum** parameter. The default optimization method used is **adadelta**, it gives a different learning rate for each parameter and is adaptive (change during training). For a \"live\" comparison between some training methods see this link http://cs.stanford.edu/people/karpathy/convnetjs/demo/trainers.html. If you use 'sgd', 'momentum' or 'nesterov_mometum' the **learning_rate** parameter  should be well tuned, .\n",
    "\n",
    "Some methods:\n",
    "\n",
    "- 'sgd' (sgd in the link), it is stochastic gradient descent, use only the learning_rate\n",
    "- 'adadelta' (adadelta in the link), use only the learning_rate parameter as a global learning rate.\n",
    "- 'nesterov_momentum' (nesterov in the link), use **learning_rate** and **momentum** parameters\n",
    "- 'momentum' (sgd+momentum in te link), use **learning_rate** and **momentum** parameters\n",
    "\n",
    "**learning_rate=1.** is the **default**, you should tune it, try values in logarithmic scale : **10^(-a)** where a is an integer.\n",
    "\n",
    "**momentum** usually take values like 0.5, 0.9, 0.95 and 0.99\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "      0          0.138792      0.528456          0.769676\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "      1          0.158759      0.476097          0.782407\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "      2           0.16171      0.462294           0.78559\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "      3          0.162457      0.453476          0.789931\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "      4          0.162679      0.446757          0.796007\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "      5          0.162768      0.440854          0.796875\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "      6          0.162469      0.435644            0.7989\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "      7          0.162183      0.431063          0.801505\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "      8          0.161936      0.426923          0.804109\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "      9          0.161933      0.423103          0.805266\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     10          0.161759      0.419437          0.804977\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     11          0.161062      0.415668          0.804688\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     12          0.160783      0.412355          0.807292\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     13          0.160908      0.409112          0.809606\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     14           0.16073      0.405911            0.8125\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     15          0.160877      0.402892          0.813947\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     16          0.161101      0.400262          0.815104\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     17          0.161279      0.397707          0.817708\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     18          0.161381      0.395266          0.818287\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     19          0.161584      0.392772          0.822049\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     20          0.161896      0.390435          0.824074\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     21          0.162135         0.388          0.825521\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     22          0.162474      0.385974          0.828704\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     23          0.162824      0.383767          0.829572\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     24          0.163175      0.381776          0.833623\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     25          0.163631      0.379755          0.836227\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     26          0.163895       0.37793          0.836806\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     27          0.163954      0.376106          0.838542\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     28          0.163469      0.374032           0.83941\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     29          0.163914      0.372056          0.840278\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('neuralnet', <lasagne.easy.SimpleNeuralNet object at 0x10d96cd90>)])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('neuralnet', SimpleNeuralNet(nb_hidden_list=[100],\n",
    "                                      max_nb_epochs=30,\n",
    "                                      batch_size=10,\n",
    "                                      learning_rate=0.01,\n",
    "                                      momentum=0.5,\n",
    "                                      optimization_method=\"nesterov_momentum\",\n",
    "                                      verbose = 1,\n",
    "                                      ))\n",
    "        ])\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch size\n",
    "\n",
    "Should be tuned well too, normally a small batch_size is better because it performs more updates per epoch (e.g with batch_size=1, there is an update for **each example**), but if you want to benefit from parallelism you would want to use bigger batch_size to speed up computations, but of course if batch_size is very big you end up doing less updates, so it is a trade-off between number of updates per epoch and computation speed, one would like an intermediate value of **batch_size**, the default value of **batch_size** is 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting learning curves\n",
    "\n",
    "If you want to compute the evolution of the loss function on a validation set, use **validation_set_ratio**, this will take a part of the training set to compute the objective/loss function and accuracy on that validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "      0          0.194675      0.532042          0.777778      0.489493          0.755787\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "      1          0.190544      0.463595          0.798225      0.471512          0.761574\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "      2          0.192026      0.442109          0.807099      0.463272          0.771991\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "      3          0.191476      0.425711          0.820602      0.459795          0.766204\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "      4          0.191966      0.409368          0.832562      0.457047          0.774306\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "      5          0.191014      0.393722          0.841821      0.459184          0.773148\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "      6          0.191047      0.378865          0.849151      0.460339          0.778935\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "      7          0.190884      0.364182          0.854552      0.464744          0.784722\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "      8          0.189846      0.350707          0.859568      0.469449          0.788194\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "      9          0.186589      0.336811           0.86034      0.479017          0.780093\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     10          0.185928      0.322736          0.866127      0.492014           0.78588\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     11          0.183964      0.308427          0.872685      0.507765           0.78588\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     12          0.182029      0.293805          0.880015      0.521923          0.768519\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     13          0.184546      0.282298          0.890818      0.539661           0.77662\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     14          0.181484      0.268068          0.894676      0.559571          0.769676\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     15          0.179535      0.257273          0.900077      0.585871          0.773148\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     16          0.174506       0.24394          0.902392       0.61317          0.770833\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     17           0.17096      0.232272          0.907407      0.639956           0.77662\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     18          0.169591      0.222224          0.908951      0.680082          0.763889\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     19          0.161361       0.20684          0.915509      0.706073          0.762731\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     20          0.155025      0.197432           0.91821      0.741221          0.766204\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     21          0.153869        0.1867           0.92284       0.75949          0.766204\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     22          0.152972      0.177859          0.920139      0.787682          0.765046\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     23          0.150054      0.166589          0.926698      0.832664          0.766204\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     24          0.147851      0.157989          0.928627      0.882083          0.769676\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     25          0.143048      0.149929          0.926698      0.938375          0.762731\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     26          0.137517       0.14096           0.93287      0.967373          0.759259\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     27          0.139381      0.132656           0.94213      0.979451          0.768519\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     28          0.137528       0.12619          0.941744       1.05209          0.761574\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     29          0.132344      0.117226          0.944059       1.06415          0.769676\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     30          0.130792      0.110993          0.947531        1.1166          0.759259\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     31          0.128503       0.10514          0.946373       1.15084          0.762731\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     32          0.130667      0.104402          0.949074       1.21675          0.763889\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     33          0.125177     0.0929277          0.946373       1.26086          0.759259\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     34          0.125859     0.0894173          0.947917       1.29902          0.758102\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     35          0.126887     0.0847902           0.94946       1.33448          0.762731\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     36           0.12007     0.0812927          0.947145       1.40589          0.762731\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     37          0.118775     0.0789316          0.954861       1.37417          0.756944\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     38          0.110332     0.0743705          0.957176       1.41189          0.760417\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     39          0.119447     0.0725006          0.959491       1.42763          0.762731\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     40          0.114183     0.0679712          0.959877        1.4663          0.758102\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     41          0.114491     0.0619843          0.962963       1.49379          0.766204\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     42         0.0946471     0.0560424          0.963735       1.56002          0.753472\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     43          0.095324     0.0557855          0.965278       1.53422          0.768519\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     44          0.096026     0.0525607          0.972222       1.54569          0.782407\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     45         0.0840871     0.0491295          0.966435       1.67193          0.767361\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     46          0.102057     0.0496003          0.968364       1.63422          0.770833\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     47          0.086674      0.044321          0.969522       1.68121          0.774306\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     48         0.0735068     0.0405996          0.957948       1.81893          0.756944\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     49         0.0690214      0.039221          0.971451       1.79081          0.769676\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     50         0.0943554     0.0472283           0.97608        1.7731          0.769676\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     51         0.0622088     0.0346276          0.969522        1.7501           0.77662\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     52         0.0740246     0.0339735          0.969907       1.89672          0.770833\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     53         0.0802312     0.0331458           0.97338       1.88662          0.770833\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     54          0.067733     0.0333354          0.981481       1.84039          0.775463\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     55          0.107309     0.0367874          0.980324        1.8674          0.782407\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     56          0.114574     0.0364099          0.974537       1.92111          0.775463\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     57         0.0804814     0.0277211          0.981867        1.8939          0.766204\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     58         0.0837344      0.024862          0.967978        2.0602          0.765046\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     59         0.0783977      0.029855          0.978781       1.93193           0.77662\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     60         0.0705761     0.0258773          0.984954       1.92457          0.769676\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     61         0.0675027     0.0243648          0.979938       1.99412          0.766204\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     62         0.0440298     0.0192947          0.985725       1.98139          0.782407\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     63         0.0662876     0.0220922          0.979167       2.05138           0.77662\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     64          0.075865     0.0236502          0.979167        2.1424          0.777778\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     65         0.0470405     0.0155296          0.986111       2.10923           0.77662\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     66         0.0662151     0.0235266          0.982253        2.2267          0.768519\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     67         0.0527603     0.0187937          0.985725       2.17034           0.78125\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     68         0.0674968     0.0199735          0.985725       2.19225           0.78125\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     69          0.234788     0.0289317          0.989969       2.15542          0.783565\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     70          0.116469     0.0190461          0.987654       2.17091           0.78125\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     71          0.131742     0.0194976          0.990355       2.19398          0.791667\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     72           0.13904     0.0186994          0.989583       2.24204          0.784722\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     73         0.0838615     0.0159392          0.991898        2.2255          0.793981\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     74         0.0718717     0.0143382          0.992284       2.23757           0.78588\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     75         0.0797216     0.0137404          0.992284       2.28767          0.789352\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     76         0.0804498     0.0143403          0.991898       2.29321          0.791667\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     77         0.0518468     0.0111821           0.99267        2.2737          0.788194\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     78         0.0418659     0.0126637          0.987654       2.27641           0.78125\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     79         0.0482387     0.0127181          0.988812       2.32704          0.784722\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     80          0.022133    0.00808253           0.99267        2.3694          0.782407\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     81         0.0362353    0.00847958           0.99537       2.40358          0.789352\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     82         0.0277715    0.00760781          0.996528       2.41012          0.791667\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     83         0.0329533    0.00724558          0.998457       2.42041          0.795139\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     84         0.0224399    0.00646055          0.998071       2.43328          0.789352\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     85         0.0201388    0.00549478          0.991512       2.50211           0.78588\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     86         0.0426879        0.0112          0.993827       2.44923          0.788194\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     87         0.0309623    0.00857095          0.996914       2.41493          0.773148\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     88         0.0617805     0.0107276          0.998071       2.64461           0.78125\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     89          0.023986    0.00536086          0.994985       2.76049          0.784722\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     90         0.0238531    0.00440352          0.998843       2.66119           0.78588\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     91         0.0247249    0.00426186          0.995756       2.70346          0.792824\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     92          0.011752    0.00299143          0.998457       2.61391          0.796296\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     93         0.0219956    0.00463058          0.998071       2.68919          0.792824\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     94         0.0159198    0.00354517          0.999228       2.65637          0.789352\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     95          0.127664    0.00928334          0.998457       2.69561          0.787037\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     96         0.0795581    0.00689911          0.998843       2.70203          0.787037\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     97         0.0822914    0.00634975          0.998843       2.76825          0.789352\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     98         0.0694291    0.00608026          0.999228       2.76685          0.787037\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     99         0.0561453    0.00475405          0.999228       2.80648          0.787037\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('neuralnet', <lasagne.easy.SimpleNeuralNet object at 0x10e3a1dd0>)])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('neuralnet', SimpleNeuralNet(nb_hidden_list=[100, 100],\n",
    "                                      max_nb_epochs=100,\n",
    "                                      batch_size=10,\n",
    "                                      learning_rate=0.9,\n",
    "                                      #L1_factor=0.001,\n",
    "                                      validation_set_ratio=0.25,\n",
    "                                      verbose = 1,\n",
    "                                      ))\n",
    "        ])\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEKCAYAAAAYd05sAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n0cBHIpFIg/J/lbeDdVJeDXcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xa1b0dfec>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stats = clf.steps[-1][1]._stats # the neural net stats\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(get_stat(\"loss_train\", stats), label=\"train loss\")\n",
    "plt.plot(get_stat(\"loss_valid\", stats), label=\"valid loss\")\n",
    "plt.title(\"loss over epochs\")\n",
    "plt.legend(loc='best')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(get_stat(\"accuracy_train\", stats), label=\"train accuracy\")\n",
    "plt.plot(get_stat(\"accuracy_valid\", stats), label=\"valid accuracy\")\n",
    "plt.title(\"accuracy over epochs\")\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "one way to deal with regularization is to penalize large weights (**weight decay)**, two forms of weight decay penalties are L1 and L2, L1 penalizes the absolute value of the weights while L2 penalizes the squared value of the weights. L1 tends to give sparse values to the weights.\n",
    "\n",
    "the parameters **L1_factor** and **L2_factor** implement weight decay. Like **activations**, they are either a list with the same size than **nb_hidden_list** giving the coeficient of regularization in each layer, or one number and in that case the same coeficient will be used everywhere.  The regularization coeficient is controlling how much you give importance to regularization compared with fitting the training data (the objective function), it must be a positive real number and it is usually very small, e.g 0.0001.\n",
    "\n",
    "By default, no weight decay is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "      0          0.310289       1.17082          0.778935      0.827278          0.751157\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "      1            0.1774      0.693522          0.782407      0.627225          0.761574\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "      2          0.169674      0.580446          0.789738       0.57323          0.768519\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "      3          0.170328      0.543482          0.790123      0.553986          0.760417\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "      4          0.172122      0.525654          0.794753      0.542172          0.762731\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "      5          0.173546       0.51507          0.794753      0.537623          0.761574\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "      6          0.174426      0.509142          0.798225       0.53418          0.761574\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "      7          0.174937      0.503893          0.800154       0.53037          0.763889\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "      8          0.175209      0.498578          0.798997      0.528194          0.771991\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "      9          0.175992      0.494395          0.802469      0.526234          0.775463\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     10          0.176737      0.490983          0.802469      0.524962          0.770833\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     11          0.176568      0.488539          0.805941      0.525021          0.770833\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     12          0.177333      0.487315          0.804784      0.524259          0.771991\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     13          0.177939       0.48541          0.806327      0.524656          0.773148\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     14          0.177909      0.483909           0.80517      0.523915          0.775463\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     15          0.177596      0.482957          0.805556      0.523666           0.77662\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     16          0.177524      0.481486          0.808256      0.524568          0.777778\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     17          0.178122      0.480548          0.806713      0.526459           0.78125\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     18           0.17827       0.48033          0.808256        0.5261          0.782407\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     19          0.178754      0.479528          0.810571      0.527063           0.78125\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     20          0.178564       0.47896          0.812114      0.527108          0.782407\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     21           0.17855      0.477935          0.813657      0.529008           0.78125\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     22          0.178495        0.4772          0.814429      0.529571           0.78125\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     23          0.178843      0.476578           0.81713      0.531001          0.783565\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     24          0.178832      0.476393          0.818673      0.532663           0.78588\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     25          0.179064      0.476019          0.818287      0.533673          0.784722\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     26          0.178582      0.475593          0.818673      0.534858          0.788194\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     27          0.178503      0.475196          0.817901      0.537416           0.78588\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     28          0.178837      0.475055          0.817901      0.537082          0.780093\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     29           0.17824       0.47449          0.818287      0.537725          0.782407\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     30          0.179393      0.474274          0.820988      0.537382           0.78125\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     31          0.179564      0.473408          0.821759      0.538207          0.780093\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     32          0.179696      0.473534          0.823688      0.537727          0.777778\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     33          0.180235      0.473088          0.825617      0.539449          0.780093\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     34          0.179505      0.472733          0.822917      0.539787          0.780093\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     35           0.17955      0.472814          0.821373      0.540067          0.782407\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     36          0.179712      0.472308          0.819444      0.543172          0.778935\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     37          0.179788       0.47208          0.817901      0.543059           0.78125\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     38          0.179214      0.472211          0.823688      0.539695          0.775463\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     39          0.179584      0.471764          0.821373      0.544243           0.77662\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     40          0.179058      0.471603          0.821759      0.543593          0.775463\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     41          0.178832       0.47212          0.820988      0.545802          0.778935\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     42          0.179303      0.472171          0.821759      0.546804          0.778935\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     43           0.17885      0.472049          0.821759      0.546959          0.775463\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     44          0.179276      0.472038          0.820216      0.547903          0.777778\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     45          0.178779      0.472304          0.822531      0.547012          0.770833\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     46          0.178767       0.47233          0.820602       0.55011          0.777778\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     47          0.179336      0.472317          0.822145      0.551387          0.775463\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     48          0.178895      0.472633          0.820216      0.552677          0.777778\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     49          0.178437      0.472562          0.822145      0.553714          0.780093\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     50          0.179105      0.472845          0.820216      0.553833          0.777778\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     51          0.177389      0.472808          0.820988      0.554383          0.773148\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     52          0.178116      0.472722          0.822145      0.553221           0.77662\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     53          0.177698      0.472644          0.821373      0.555325          0.780093\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     54          0.178113      0.472467          0.820602      0.555338           0.78125\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     55          0.178196      0.472719          0.821373      0.557222          0.778935\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     56          0.178219      0.471924          0.820988      0.557955          0.780093\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     57          0.178216       0.47243          0.823688      0.557071           0.78125\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     58          0.178271      0.472275          0.822531      0.557338          0.783565\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     59          0.178207      0.472473          0.822145      0.560671           0.78125\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     60          0.178057      0.471996          0.822531       0.55799          0.778935\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     61          0.178273      0.472082          0.823688      0.562296          0.777778\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     62          0.177983      0.471802          0.823688      0.559865          0.780093\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     63          0.178826      0.471364          0.822531      0.564046          0.780093\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     64          0.178836      0.472099          0.824846      0.565525          0.780093\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     65          0.179026      0.472019          0.823688      0.566508          0.778935\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     66          0.179495      0.471901          0.824074      0.567334          0.778935\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     67          0.180048      0.472597          0.823688      0.567493          0.778935\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     68          0.178739      0.472431          0.824074      0.567094          0.777778\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     69          0.179168      0.472539          0.824846      0.567122          0.780093\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     70           0.17927      0.472929          0.823688      0.567531           0.78125\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     71           0.17954      0.473118          0.827932      0.568273           0.78125\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     72          0.178685      0.473054          0.825617      0.567977           0.78125\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     73          0.178841      0.473075          0.826389      0.569476          0.784722\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     74          0.178153      0.473556           0.82716      0.570451          0.782407\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     75          0.178479      0.473629          0.826775      0.570851          0.783565\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     76          0.178775      0.474021          0.827932      0.570835          0.783565\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     77          0.179014      0.473968           0.82716      0.570981          0.784722\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     78          0.178491      0.474215          0.826775      0.570843          0.784722\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     79          0.178735      0.474815          0.826389      0.572478          0.782407\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     80          0.178134      0.474521          0.826003      0.573117          0.783565\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     81          0.178222       0.47509          0.826003      0.573261          0.782407\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     82          0.179118      0.475446           0.82716      0.573589          0.787037\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     83          0.178104       0.47495          0.828704      0.573331           0.78588\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     84          0.177755      0.475549          0.829475      0.574004          0.787037\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     85          0.178126      0.475038          0.827932      0.574425          0.787037\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     86           0.17821      0.475097           0.82909       0.57535           0.78588\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     87          0.178375      0.475326          0.827546      0.575731          0.787037\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     88          0.178134      0.475622          0.829475      0.575692          0.784722\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     89          0.177563      0.475132          0.831019      0.577488          0.783565\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     90          0.177367      0.475689          0.831019      0.575744          0.787037\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     91          0.177005      0.475453          0.830633      0.577413          0.783565\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     92          0.178274      0.475846          0.831404      0.577152          0.784722\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     93          0.178186       0.47552          0.831404      0.578803          0.784722\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     94          0.177913      0.475941           0.83179      0.580268          0.784722\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     95          0.178143      0.476125          0.833333      0.580597          0.783565\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     96           0.17813      0.476419          0.831404      0.583074          0.783565\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     97          0.178318      0.476972          0.831019       0.58574          0.782407\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     98           0.17731      0.476747          0.832562      0.585705           0.78125\n",
      "  epoch    loss_train_std    loss_train    accuracy_train    loss_valid    accuracy_valid\n",
      "-------  ----------------  ------------  ----------------  ------------  ----------------\n",
      "     99          0.177988      0.477317          0.832562      0.587475           0.78125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('neuralnet', <lasagne.easy.SimpleNeuralNet object at 0xa13dfdac>)])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "clf = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('neuralnet', SimpleNeuralNet(nb_hidden_list=[100, 100],\n",
    "                                      max_nb_epochs=100,\n",
    "                                      batch_size=10,\n",
    "                                      learning_rate=0.9,\n",
    "                                      L1_factor=[0.0007, 0.0007],\n",
    "                                      L2_factor=[0.0007,0.0007],\n",
    "                                      verbose = 1,\n",
    "                                      validation_set_ratio=0.25,\n",
    "                                      ))\n",
    "        ])\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEKCAYAAAAYd05sAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGpRo4CORSKRB+T9I4pgh+Cy1rQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10c04d550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stats = clf.steps[-1][1]._stats # the neural net stats\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(get_stat(\"loss_train\", stats), label=\"train loss\")\n",
    "plt.plot(get_stat(\"loss_valid\", stats), label=\"valid loss\")\n",
    "plt.title(\"loss over epochs\")\n",
    "plt.legend(loc='best')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(get_stat(\"accuracy_train\", stats), label=\"train accuracy\")\n",
    "plt.plot(get_stat(\"accuracy_valid\", stats), label=\"valid accuracy\")\n",
    "plt.title(\"accuracy over epochs\")\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Visualization of the weights\n",
    "\n",
    "\n",
    "one way to visualize the weights learned by the neural network is to use the **Hinton diagram** (http://wiki.scipy.org/Cookbook/Matplotlib/HintonDiagrams). Each box is a weight value, the color show the sign of the weight (two different colors are used) and the size of the boxes is the magnitude of the weight.\n",
    "In our case a **green box** is a positive weight while a **red box** is a negative weight\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "      0          0.108755      0.603862            0.7636\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "      1         0.0560564      0.495617          0.779803\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "      2         0.0575859      0.468474          0.792824\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "      3         0.0566145      0.455937          0.798032\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "      4         0.0569944      0.447127          0.801794\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "      5         0.0581024      0.439526          0.804398\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "      6          0.057824       0.43333          0.811053\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "      7         0.0552128      0.428314          0.809028\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "      8         0.0548162      0.422936          0.814236\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "      9         0.0561611      0.417661          0.814236\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     10         0.0576931      0.413778          0.813947\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     11         0.0571146       0.41321          0.812789\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     12         0.0589206      0.403878          0.817998\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     13         0.0568518      0.403591            0.8125\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     14         0.0562503       0.39911          0.809028\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     15           0.05316      0.401211           0.81713\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     16         0.0530493      0.394497          0.823206\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     17         0.0504261      0.394224           0.81713\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     18         0.0594372      0.392753          0.811343\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     19         0.0595866      0.401924          0.809317\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     20         0.0555976      0.395335          0.811632\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     21         0.0548648      0.390298          0.815683\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     22         0.0559756      0.391074          0.812789\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     23         0.0559416      0.387658          0.817419\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     24          0.058357      0.383225          0.820891\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     25         0.0549273      0.380302          0.819155\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     26         0.0585385      0.383183          0.815683\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     27         0.0571329      0.379693          0.816262\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     28         0.0570552      0.382691          0.817708\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     29         0.0608205      0.374036          0.821759\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     30         0.0656143      0.374204          0.823206\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     31         0.0619079       0.38294          0.816551\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     32         0.0618727      0.383494          0.816551\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     33         0.0664571      0.383577          0.822049\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     34         0.0680291      0.378907          0.821181\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     35         0.0648479      0.375816           0.82147\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     36         0.0681008      0.372882          0.826968\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     37         0.0637408      0.369977          0.819155\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     38         0.0642295      0.372914            0.8261\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     39         0.0690412      0.366759          0.822627\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     40         0.0636246      0.363179          0.818287\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     41         0.0704897      0.374771          0.815972\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     42         0.0627964      0.368866          0.822627\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     43         0.0706567      0.369367          0.823785\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     44         0.0687536      0.369408          0.831019\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     45         0.0653839      0.369112           0.83015\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     46         0.0692795      0.366335          0.833333\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     47         0.0660465      0.364533          0.829861\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     48         0.0660041      0.365064          0.831887\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     49         0.0698219      0.368822           0.82581\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     50         0.0700762      0.366628          0.823206\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     51         0.0676607      0.362174          0.826389\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     52         0.0666256      0.361891          0.824653\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     53         0.0680544      0.359191          0.820023\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     54         0.0632942      0.361122          0.828993\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     55         0.0638908      0.360441          0.820023\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     56         0.0649971      0.364308          0.824653\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     57         0.0737625      0.365153          0.828414\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     58         0.0645736      0.361057           0.82581\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     59         0.0659076       0.36425          0.820312\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     60         0.0674436      0.359362          0.824074\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     61         0.0640162      0.359908          0.827257\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     62         0.0633267      0.361788          0.820891\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     63         0.0679759      0.364782          0.824942\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     64         0.0688866       0.36358          0.822049\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     65         0.0592744      0.358955           0.82581\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     66         0.0642909      0.358127           0.82581\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     67          0.067209      0.360126          0.822627\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     68          0.061151      0.362931          0.833623\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     69           0.06529      0.361735          0.820312\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     70         0.0714727        0.3712          0.827257\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     71         0.0638928      0.358469          0.824074\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     72         0.0640254       0.36309          0.825521\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     73         0.0635827      0.360044            0.8261\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     74          0.065017      0.360203          0.820312\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     75         0.0682564      0.365499          0.827546\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     76         0.0601009      0.359116          0.822627\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     77         0.0718178      0.367214          0.822049\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     78         0.0639223      0.368547          0.829282\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     79         0.0624299      0.359474          0.822917\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     80         0.0645492      0.364393          0.826389\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     81         0.0634301      0.363151           0.82147\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     82         0.0681257      0.364533          0.822338\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     83         0.0667685      0.366336          0.832465\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     84          0.063811      0.364144          0.825231\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     85         0.0631452      0.360914          0.826968\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     86         0.0629485      0.358683          0.824074\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     87         0.0634703       0.35271           0.83044\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     88         0.0699663      0.357335          0.820312\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     89         0.0623678       0.36321          0.833044\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     90         0.0625751      0.359193          0.831887\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     91         0.0596456      0.352712          0.832176\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     92         0.0653841      0.358024          0.822338\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     93         0.0639872        0.3629           0.83044\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     94         0.0608353      0.356517          0.822917\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     95          0.065756      0.357308           0.82581\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     96         0.0603699      0.360166          0.828125\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     97         0.0663282      0.353465          0.825521\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     98         0.0648501      0.356061          0.824074\n",
      "  epoch    loss_train_std    loss_train    accuracy_train\n",
      "-------  ----------------  ------------  ----------------\n",
      "     99         0.0617298      0.359626          0.826389\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('neuralnet', <lasagne.easy.SimpleNeuralNet object at 0x10fdade10>)])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('neuralnet', SimpleNeuralNet(nb_hidden_list=[10, 10],\n",
    "                                      max_nb_epochs=100,\n",
    "                                      batch_size=100,\n",
    "                                      learning_rate=0.1,\n",
    "                                      momentum=0.95,\n",
    "                                      optimization_method=\"nesterov_momentum\",\n",
    "                                      verbose = 1,\n",
    "                                      ))\n",
    "        ])\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWwAAAEKCAYAAAA2Mm/+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nA22LVwxln4ttAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x111d58c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from lasagne.misc.hinton import hinton\n",
    "from IPython.html.widgets import interact\n",
    "import matplotlib.pyplot as plt\n",
    "nnet = clf.steps[-1][1]\n",
    "\n",
    "# Note that we have an additional layer here (the last) which is the output layer\n",
    "\n",
    "@interact\n",
    "def plot_weights(hidden_layer=(1, len(nnet._layers)-1)):\n",
    "    layer = nnet._layers[hidden_layer].W.get_value()\n",
    "    plt.clf()\n",
    "    hinton(layer, fig=plt.gcf())\n",
    "    plt.title(\"number of inputs:{0}, number of outputs: {1}\".format(layer.shape[0], layer.shape[1]))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
